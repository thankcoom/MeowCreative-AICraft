#!/usr/bin/env python3
"""
Helpful Content Analyzer - æœ‰åƒ¹å€¼å…§å®¹åˆ†æå™¨
åŸºæ–¼ Google Helpful Content System (2025)

Google Helpful Content System æ ¸å¿ƒåŸå‰‡ï¼š
- People-first contentï¼ˆç”¨æˆ¶å„ªå…ˆï¼Œè€Œéæœå°‹å¼•æ“å„ªå…ˆï¼‰
- çœŸå¯¦ç¶“é©—å’Œç¬¬ä¸€æ‰‹çŸ¥è­˜
- é¿å…ç‚ºæ’åè€Œå‰µä½œ
- æä¾›å¯¦è³ªåƒ¹å€¼

2025 å¹´èˆ‡æ ¸å¿ƒæ›´æ–°ã€è©•è«–æ¼”ç®—æ³•æ•´åˆç‚ºå–®ä¸€å¼•æ“
ç›®æ¨™ï¼šæ¸›å°‘ 45% ä½å“è³ªå…§å®¹

Version: 1.0.0
Date: 2025-11-04
"""

import re
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass


@dataclass
class HelpfulnessScore:
    """æœ‰åƒ¹å€¼åº¦è©•åˆ†çµæœ"""
    overall_score: float  # ç¸½åˆ† (0-100)
    people_first_score: float  # ç”¨æˆ¶å„ªå…ˆåˆ†æ•¸
    value_density_score: float  # åƒ¹å€¼å¯†åº¦åˆ†æ•¸
    actionability_score: float  # å¯æ“ä½œæ€§åˆ†æ•¸
    authenticity_score: float  # çœŸå¯¦æ€§åˆ†æ•¸
    red_flags: List[str]  # ç´…æ——è­¦å‘Š
    green_signals: List[str]  # æ­£é¢ä¿¡è™Ÿ
    quality_status: str  # excellent/good/acceptable/poor


class HelpfulContentAnalyzer:
    """æœ‰åƒ¹å€¼å…§å®¹åˆ†æå™¨"""

    def __init__(self):
        # SEO éåº¦å„ªåŒ–çš„é—œéµè©ï¼ˆè² é¢ä¿¡è™Ÿï¼‰
        self.seo_spam_keywords = [
            'æ’åç¬¬ä¸€', 'æœ€å¥½çš„', 'æœ€ä½³', 'é ‚ç´š', 'å† è»',
            'çµ•å°', 'ä¿è­‰', '100%', 'å®Œç¾', 'ç¥å™¨',
            'ç§’æ®º', 'çµ‚æ¥µ', 'å²ä¸Šæœ€å¼·'
        ]

        # æœ‰åƒ¹å€¼å…§å®¹çš„æ­£é¢ä¿¡è™Ÿ
        self.value_signals = [
            'æˆ‘çš„ç¶“é©—', 'å¯¦æ¸¬', 'è¦ªè‡ªè©¦é', 'å¯¦éš›ä½¿ç”¨',
            'èˆ‰ä¾‹ä¾†èªª', 'æ¯”å¦‚', 'å…·é«”', 'æ­¥é©Ÿ', 'æ–¹æ³•',
            'å„ªé»æ˜¯', 'ç¼ºé»æ˜¯', 'éœ€è¦æ³¨æ„', 'å»ºè­°'
        ]

    def analyze(
        self,
        article_path: str,
        experience_profile_path: Optional[str] = None
    ) -> HelpfulnessScore:
        """
        åˆ†ææ–‡ç« çš„æœ‰åƒ¹å€¼åº¦

        Args:
            article_path: æ–‡ç« æª”æ¡ˆè·¯å¾‘
            experience_profile_path: ç¶“é©—æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            HelpfulnessScore: æœ‰åƒ¹å€¼åº¦è©•åˆ†
        """
        # è®€å–æ–‡ç« 
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # è®€å–ç¶“é©—æª”æ¡ˆï¼ˆå¦‚æœæœ‰ï¼‰
        experience_data = None
        if experience_profile_path and Path(experience_profile_path).exists():
            with open(experience_profile_path, 'r', encoding='utf-8') as f:
                experience_data = f.read()

        # æ¸…ç†å…§å®¹
        clean_content = self._clean_content(content)

        # åˆ†æ 1: People-First vs SEO-First
        people_first_score, pf_flags, pf_signals = self._analyze_people_first(clean_content)

        # åˆ†æ 2: åƒ¹å€¼å¯†åº¦ï¼ˆå¯¦è³ªå…§å®¹ vs ç©ºè©±ï¼‰
        value_density_score, vd_flags, vd_signals = self._analyze_value_density(clean_content)

        # åˆ†æ 3: å¯æ“ä½œæ€§ï¼ˆè®€è€…èƒ½å¦æ¡å–è¡Œå‹•ï¼‰
        actionability_score, act_flags, act_signals = self._analyze_actionability(clean_content)

        # åˆ†æ 4: çœŸå¯¦æ€§ï¼ˆåŸºæ–¼ç¶“é©—æª”æ¡ˆï¼‰
        authenticity_score, auth_flags, auth_signals = self._analyze_authenticity(
            clean_content,
            experience_data
        )

        # åˆä½µæ‰€æœ‰ç´…æ——å’Œæ­£é¢ä¿¡è™Ÿ
        all_red_flags = pf_flags + vd_flags + act_flags + auth_flags
        all_green_signals = pf_signals + vd_signals + act_signals + auth_signals

        # è¨ˆç®—ç¸½åˆ†
        overall_score = self._calculate_overall_score(
            people_first_score,
            value_density_score,
            actionability_score,
            authenticity_score
        )

        # åˆ¤æ–·å“è³ªç‹€æ…‹
        quality_status = self._assess_quality(overall_score, len(all_red_flags))

        return HelpfulnessScore(
            overall_score=overall_score,
            people_first_score=people_first_score,
            value_density_score=value_density_score,
            actionability_score=actionability_score,
            authenticity_score=authenticity_score,
            red_flags=all_red_flags,
            green_signals=all_green_signals,
            quality_status=quality_status
        )

    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å…§å®¹"""
        # ç§»é™¤ frontmatter
        content = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)

        # ä¿ç•™ç¨‹å¼ç¢¼å€å¡Šï¼ˆå› ç‚ºæ•™å­¸å…§å®¹éœ€è¦ï¼‰ä½†æ¨™è¨˜
        # content = re.sub(r'```.*?```', '[CODE_BLOCK]', content, flags=re.DOTALL)

        return content.strip()

    def _analyze_people_first(self, content: str) -> Tuple[float, List[str], List[str]]:
        """
        åˆ†ææ˜¯å¦ç‚ºç”¨æˆ¶å„ªå…ˆå…§å®¹

        æª¢æ¸¬ï¼š
        - éåº¦ SEO å„ªåŒ–ï¼ˆé—œéµå­—å †ç Œï¼‰
        - æ¨™é¡Œé»¨
        - ç‚ºæ’åè€Œå¯«çš„è·¡è±¡
        """
        red_flags = []
        green_signals = []
        score = 100.0

        # æª¢æ¸¬ 1: SEO åƒåœ¾é—œéµè©
        spam_found = []
        for keyword in self.seo_spam_keywords:
            if keyword in content:
                spam_found.append(keyword)

        if len(spam_found) > 3:
            score -= 20
            red_flags.append(f"ğŸš© éåº¦ä½¿ç”¨ SEO èª‡å¤§è©å½™: {', '.join(spam_found[:5])}")
        elif len(spam_found) > 0:
            score -= 10
            red_flags.append(f"âš ï¸ ä½¿ç”¨äº†éƒ¨åˆ†èª‡å¤§è©å½™: {', '.join(spam_found)}")

        # æª¢æ¸¬ 2: é—œéµå­—å¯†åº¦éé«˜ï¼ˆå¯èƒ½æ˜¯é—œéµå­—å †ç Œï¼‰
        # ç°¡åŒ–ï¼šæª¢æŸ¥æŸå€‹è©å½™çš„éåº¦é‡è¤‡
        words = content.split()
        word_freq = {}
        for word in words:
            if len(word) > 2:
                word_freq[word] = word_freq.get(word, 0) + 1

        max_freq = max(word_freq.values()) if word_freq else 0
        if len(words) > 0 and max_freq / len(words) > 0.05:
            score -= 15
            most_common = max(word_freq, key=word_freq.get)
            red_flags.append(
                f"ğŸš© é—œéµå­—éåº¦é‡è¤‡: '{most_common}' å‡ºç¾ {max_freq} æ¬¡ "
                f"({max_freq/len(words):.1%}ï¼Œå»ºè­° < 5%)"
            )

        # æª¢æ¸¬ 3: æ¨™é¡Œé»¨æ¨¡å¼
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            title = title_match.group(1)
            clickbait_patterns = ['ä½ ä¸çŸ¥é“', 'éœ‡é©š', 'çµ•å°', 'å¿…çœ‹', 'å¿…è®€', 'ä¸çœ‹å¾Œæ‚”']
            if any(pattern in title for pattern in clickbait_patterns):
                score -= 15
                red_flags.append(f"ğŸš© æ¨™é¡Œå¯èƒ½æ˜¯æ¨™é¡Œé»¨: {title}")

        # æ­£é¢ä¿¡è™Ÿï¼šä½¿ç”¨ç”¨æˆ¶å°å‘èªè¨€
        user_oriented = ['ä½ å¯ä»¥', 'å¹«åŠ©ä½ ', 'è®“ä½ ', 'é©åˆä½ ', 'å¦‚æœä½ ', 'ä½ éœ€è¦']
        user_count = sum(1 for phrase in user_oriented if phrase in content)

        if user_count >= 5:
            green_signals.append(f"âœ… ä½¿ç”¨ç”¨æˆ¶å°å‘èªè¨€ ({user_count} è™•)")
        elif user_count >= 3:
            score += 5

        return max(0, min(100, score)), red_flags, green_signals

    def _analyze_value_density(self, content: str) -> Tuple[float, List[str], List[str]]:
        """
        åˆ†æåƒ¹å€¼å¯†åº¦

        æª¢æ¸¬ï¼š
        - ç©ºè©±æ¯”ä¾‹ï¼ˆå»¢è©±ã€é™³è©æ¿«èª¿ï¼‰
        - å¯¦è³ªå…§å®¹æ¯”ä¾‹
        - æ·±åº¦ vs å»£åº¦
        """
        red_flags = []
        green_signals = []
        score = 100.0

        # æª¢æ¸¬ 1: å¸¸è¦‹ç©ºè©±
        fluff_phrases = [
            'çœ¾æ‰€å‘¨çŸ¥', 'æ¯‹åº¸ç½®ç–‘', 'ä¸è¨€è€Œå–»', 'é¡¯è€Œæ˜“è¦‹',
            'åœ¨ç•¶ä»Šç¤¾æœƒ', 'éš¨è‘—ç§‘æŠ€çš„ç™¼å±•', 'åœ¨é€™å€‹æ™‚ä»£',
            'ç¸½çš„ä¾†èªª', 'ç¶œä¸Šæ‰€è¿°'  # é€™äº›å¯ä»¥ç”¨ï¼Œä½†éå¤šå°±æ˜¯ç©ºè©±
        ]

        fluff_count = sum(1 for phrase in fluff_phrases if phrase in content)
        total_sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content))

        if total_sentences > 0 and fluff_count / total_sentences > 0.15:
            score -= 20
            red_flags.append(
                f"ğŸš© ç©ºè©±æ¯”ä¾‹éé«˜: {fluff_count} / {total_sentences} "
                f"({fluff_count/total_sentences:.1%}ï¼Œå»ºè­° < 15%)"
            )

        # æª¢æ¸¬ 2: å¯¦è³ªå…§å®¹ä¿¡è™Ÿï¼ˆæ•¸æ“šã€æ¡ˆä¾‹ã€æ­¥é©Ÿï¼‰
        value_signals_found = []

        # æ•¸æ“š
        numbers = len(re.findall(r'\d+[%ï¼…]|\d+\.\d+|\d+å€‹|ç¬¬\d+', content))
        if numbers >= 5:
            value_signals_found.append(f"æ•¸æ“š ({numbers} è™•)")

        # æ¡ˆä¾‹/ä¾‹å­
        examples = len(re.findall(r'ä¾‹å¦‚|æ¯”å¦‚|èˆ‰ä¾‹|æ¡ˆä¾‹|ç¯„ä¾‹|å¯¦ä¾‹', content))
        if examples >= 3:
            value_signals_found.append(f"æ¡ˆä¾‹ ({examples} è™•)")

        # æ­¥é©Ÿ
        steps = len(re.findall(r'æ­¥é©Ÿ|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ­¥|^\d+\.|^[-*]\s', content, re.MULTILINE))
        if steps >= 3:
            value_signals_found.append(f"æ­¥é©Ÿ ({steps} è™•)")

        if len(value_signals_found) >= 2:
            green_signals.append(f"âœ… åŒ…å«å¯¦è³ªå…§å®¹: {', '.join(value_signals_found)}")
            score += 10
        elif len(value_signals_found) == 0:
            score -= 15
            red_flags.append("ğŸš© ç¼ºä¹å¯¦è³ªå…§å®¹ï¼ˆç„¡æ•¸æ“šã€æ¡ˆä¾‹ã€æ­¥é©Ÿï¼‰")

        # æª¢æ¸¬ 3: æ·±åº¦ï¼ˆæ–‡ç« é•·åº¦ + ç´°ç¯€ï¼‰
        word_count = len(content)

        if word_count < 500:
            score -= 20
            red_flags.append(f"ğŸš© å…§å®¹éçŸ­ ({word_count} å­—ï¼Œå»ºè­° >= 800 å­—)")
        elif word_count < 800:
            score -= 10
            red_flags.append(f"âš ï¸ å…§å®¹åçŸ­ ({word_count} å­—ï¼Œå»ºè­° >= 800 å­—)")
        elif word_count >= 1500:
            green_signals.append(f"âœ… å…§å®¹è±å¯Œ ({word_count} å­—)")
            score += 5

        return max(0, min(100, score)), red_flags, green_signals

    def _analyze_actionability(self, content: str) -> Tuple[float, List[str], List[str]]:
        """
        åˆ†æå¯æ“ä½œæ€§

        æª¢æ¸¬ï¼š
        - æ˜¯å¦æä¾›å¯åŸ·è¡Œçš„å»ºè­°
        - æ˜¯å¦æœ‰æ˜ç¢ºçš„æ­¥é©Ÿ
        - æ˜¯å¦æœ‰å¯¦ç”¨çš„å·¥å…·/è³‡æº
        """
        red_flags = []
        green_signals = []
        score = 100.0

        # æª¢æ¸¬ 1: è¡Œå‹•å°å‘èªè¨€
        action_verbs = [
            'å¯ä»¥', 'å»ºè­°', 'è©¦è©¦', 'å˜—è©¦', 'ä½¿ç”¨', 'åŸ·è¡Œ',
            'é–‹å§‹', 'è¨­å®š', 'é…ç½®', 'å®‰è£', 'ä¸‹è¼‰', 'é»æ“Š'
        ]

        action_count = sum(1 for verb in action_verbs if verb in content)

        if action_count >= 10:
            green_signals.append(f"âœ… åŒ…å«è±å¯Œçš„è¡Œå‹•æŒ‡å¼• ({action_count} è™•)")
            score += 10
        elif action_count >= 5:
            green_signals.append(f"âœ… åŒ…å«è¡Œå‹•æŒ‡å¼• ({action_count} è™•)")
        elif action_count < 3:
            score -= 15
            red_flags.append("ğŸš© ç¼ºä¹å¯æ“ä½œçš„å»ºè­°")

        # æª¢æ¸¬ 2: å…·é«”æ­¥é©Ÿ
        steps = len(re.findall(
            r'æ­¥é©Ÿ\s*\d+|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ­¥|^\d+\.',
            content,
            re.MULTILINE
        ))

        if steps >= 5:
            green_signals.append(f"âœ… æä¾›æ¸…æ™°çš„æ­¥é©ŸæŒ‡å¼• ({steps} å€‹æ­¥é©Ÿ)")
            score += 10
        elif steps >= 3:
            green_signals.append(f"âœ… åŒ…å«æ­¥é©Ÿèªªæ˜ ({steps} å€‹æ­¥é©Ÿ)")

        # æª¢æ¸¬ 3: å·¥å…·å’Œè³‡æº
        resources = len(re.findall(r'å·¥å…·|è»Ÿé«”|å¹³å°|ç¶²ç«™|è³‡æº|ä¸‹è¼‰|é€£çµ', content))

        if resources >= 5:
            green_signals.append(f"âœ… æä¾›å¯¦ç”¨å·¥å…·/è³‡æº ({resources} è™•)")
            score += 5

        # æª¢æ¸¬ 4: ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼ˆæŠ€è¡“æ–‡ç« çš„å¯æ“ä½œæ€§ï¼‰
        code_blocks = len(re.findall(r'```', content))

        if code_blocks >= 2:
            green_signals.append(f"âœ… åŒ…å«ç¨‹å¼ç¢¼ç¯„ä¾‹ ({code_blocks // 2} å€‹)")
            score += 10

        return max(0, min(100, score)), red_flags, green_signals

    def _analyze_authenticity(
        self,
        content: str,
        experience_data: Optional[str]
    ) -> Tuple[float, List[str], List[str]]:
        """
        åˆ†æçœŸå¯¦æ€§

        æª¢æ¸¬ï¼š
        - æ˜¯å¦æœ‰ç¬¬ä¸€æ‰‹ç¶“é©—
        - æ˜¯å¦æœ‰å€‹äººè§€é»
        - æ˜¯å¦æœ‰çœŸå¯¦çš„å„ªç¼ºé»è©•åƒ¹
        """
        red_flags = []
        green_signals = []
        score = 100.0

        # æª¢æ¸¬ 1: ç¬¬ä¸€äººç¨±ç¶“é©—
        first_person = len(re.findall(r'æˆ‘[çš„åœ¨ç”¨è©¦åš]|æˆ‘å€‘[çš„åœ¨ç”¨è©¦åš]', content))

        if first_person >= 5:
            green_signals.append(f"âœ… åŒ…å«ç¬¬ä¸€æ‰‹ç¶“é©— ({first_person} è™•)")
            score += 15
        elif first_person >= 2:
            green_signals.append(f"âœ… æœ‰å€‹äººè§€é» ({first_person} è™•)")
            score += 5
        elif first_person == 0:
            score -= 20
            red_flags.append("ğŸš© ç¼ºä¹ç¬¬ä¸€æ‰‹ç¶“é©—ï¼ˆç„¡ç¬¬ä¸€äººç¨±æ•˜è¿°ï¼‰")

        # æª¢æ¸¬ 2: çœŸå¯¦çš„å„ªç¼ºé»è©•åƒ¹
        pros_cons = (
            len(re.findall(r'å„ªé»|å¥½è™•|å„ªå‹¢|ç›Šè™•', content)) > 0 and
            len(re.findall(r'ç¼ºé»|ä¸è¶³|é™åˆ¶|ç¼ºé™·|å•é¡Œ', content)) > 0
        )

        if pros_cons:
            green_signals.append("âœ… æä¾›å¹³è¡¡çš„å„ªç¼ºé»è©•åƒ¹")
            score += 15
        else:
            score -= 15
            red_flags.append("ğŸš© ç¼ºä¹å¹³è¡¡è©•åƒ¹ï¼ˆåªæœ‰å„ªé»æˆ–åªæœ‰ç¼ºé»ï¼‰")

        # æª¢æ¸¬ 3: èˆ‡ç¶“é©—æª”æ¡ˆçš„ä¸€è‡´æ€§
        if experience_data:
            # æª¢æŸ¥æ–‡ç« æ˜¯å¦æåˆ°ç¶“é©—æª”æ¡ˆä¸­çš„é—œéµè³‡è¨Š
            # ç°¡åŒ–ï¼šæª¢æŸ¥æ˜¯å¦æœ‰æ™‚é–“åƒè€ƒ
            has_time_ref = bool(re.search(r'\d{4}å¹´|\d+å€‹æœˆ|æœ€è¿‘|ä¸Šé€±|å»å¹´', content))

            if has_time_ref:
                green_signals.append("âœ… åŒ…å«å…·é«”æ™‚é–“åƒè€ƒï¼ˆçœŸå¯¦æ€§æŒ‡æ¨™ï¼‰")
                score += 10
        else:
            # æ²’æœ‰ç¶“é©—æª”æ¡ˆï¼Œæª¢æŸ¥æ–‡ç« æœ¬èº«çš„çœŸå¯¦æ€§ä¿¡è™Ÿ
            if not re.search(r'å¯¦æ¸¬|è¦ªè‡ª|å¯¦éš›|çœŸå¯¦|é«”é©—', content):
                score -= 10
                red_flags.append("âš ï¸ ç¼ºä¹çœŸå¯¦æ€§ä¿¡è™Ÿï¼ˆå»ºè­°åŠ å…¥å¯¦æ¸¬ã€è¦ªèº«é«”é©—ï¼‰")

        # æª¢æ¸¬ 4: é¿å…éåº¦æ­£é¢æˆ–éåº¦è² é¢
        positive_words = len(re.findall(r'å¾ˆå¥½|éå¸¸å¥½|æ¥µä½³|å®Œç¾|å„ªç§€|æ¨è–¦', content))
        negative_words = len(re.findall(r'å¾ˆå·®|éå¸¸å·®|ç³Ÿç³•|ä¸æ¨è–¦|é¿å…', content))
        total_sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content))

        if total_sentences > 0:
            sentiment_ratio = abs(positive_words - negative_words) / total_sentences

            if sentiment_ratio > 0.3:
                score -= 10
                red_flags.append("âš ï¸ æƒ…ç·’éæ–¼æ¥µç«¯ï¼ˆéåº¦æ­£é¢æˆ–è² é¢ï¼‰")

        return max(0, min(100, score)), red_flags, green_signals

    def _calculate_overall_score(
        self,
        people_first: float,
        value_density: float,
        actionability: float,
        authenticity: float
    ) -> float:
        """
        è¨ˆç®—ç¸½é«”æœ‰åƒ¹å€¼åº¦åˆ†æ•¸

        æ¬Šé‡ï¼š
        - People-First: 30%ï¼ˆæœ€é‡è¦ï¼‰
        - Authenticity: 30%ï¼ˆèˆ‡ E-E-A-T å°æ‡‰ï¼‰
        - Value Density: 25%
        - Actionability: 15%
        """
        overall = (
            people_first * 0.30 +
            authenticity * 0.30 +
            value_density * 0.25 +
            actionability * 0.15
        )

        return round(overall, 1)

    def _assess_quality(self, score: float, red_flag_count: int) -> str:
        """è©•ä¼°å“è³ªç­‰ç´š"""
        # å¦‚æœæœ‰åš´é‡ç´…æ——ï¼Œé™ç´š
        if red_flag_count >= 5:
            return 'poor'
        elif red_flag_count >= 3:
            return 'acceptable' if score >= 60 else 'poor'

        # åŸºæ–¼åˆ†æ•¸è©•ç´š
        if score >= 80:
            return 'excellent'
        elif score >= 65:
            return 'good'
        elif score >= 50:
            return 'acceptable'
        else:
            return 'poor'

    def generate_markdown_report(
        self,
        score: HelpfulnessScore,
        output_path: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„æœ‰åƒ¹å€¼åº¦å ±å‘Š

        Args:
            score: æœ‰åƒ¹å€¼åº¦è©•åˆ†
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            str: Markdown æ ¼å¼å ±å‘Š
        """
        quality_emoji = {
            'excellent': 'ğŸŸ¢',
            'good': 'ğŸŸ¡',
            'acceptable': 'ğŸŸ ',
            'poor': 'ğŸ”´'
        }

        report = f"""# Helpful Content æœ‰åƒ¹å€¼åº¦åˆ†æå ±å‘Š

## ğŸ“Š ç¸½é«”è©•åˆ†

**æœ‰åƒ¹å€¼åº¦åˆ†æ•¸**: {score.overall_score}/100 {quality_emoji[score.quality_status]}

**å“è³ªç­‰ç´š**: {score.quality_status.upper()}

---

## ğŸ“ˆ å››å¤§ç¶­åº¦åˆ†æ•¸

### 1. People-First (ç”¨æˆ¶å„ªå…ˆ) - æ¬Šé‡ 30%
**{score.people_first_score:.1f}/100** {quality_emoji[self._score_to_quality(score.people_first_score)]}

æª¢æ¸¬å…§å®¹æ˜¯å¦ç‚ºç”¨æˆ¶è€Œå¯«ï¼Œè€Œéç‚º SEO æ’åè€Œå¯«

### 2. Authenticity (çœŸå¯¦æ€§) - æ¬Šé‡ 30%
**{score.authenticity_score:.1f}/100** {quality_emoji[self._score_to_quality(score.authenticity_score)]}

æª¢æ¸¬æ˜¯å¦æœ‰ç¬¬ä¸€æ‰‹ç¶“é©—å’ŒçœŸå¯¦è§€é»

### 3. Value Density (åƒ¹å€¼å¯†åº¦) - æ¬Šé‡ 25%
**{score.value_density_score:.1f}/100** {quality_emoji[self._score_to_quality(score.value_density_score)]}

æª¢æ¸¬å¯¦è³ªå…§å®¹æ¯”ä¾‹ï¼Œé¿å…ç©ºè©±

### 4. Actionability (å¯æ“ä½œæ€§) - æ¬Šé‡ 15%
**{score.actionability_score:.1f}/100** {quality_emoji[self._score_to_quality(score.actionability_score)]}

æª¢æ¸¬æ˜¯å¦æä¾›å¯åŸ·è¡Œçš„å»ºè­°å’Œæ­¥é©Ÿ

---

## âœ… æ­£é¢ä¿¡è™Ÿ ({len(score.green_signals)} å€‹)

"""

        if score.green_signals:
            for signal in score.green_signals:
                report += f"{signal}\n"
        else:
            report += "*æœªæª¢æ¸¬åˆ°é¡¯è‘—çš„æ­£é¢ä¿¡è™Ÿ*\n"

        report += "\n---\n\n## ğŸš© éœ€è¦æ”¹é€²çš„å•é¡Œ ({} å€‹)\n\n".format(len(score.red_flags))

        if score.red_flags:
            for flag in score.red_flags:
                report += f"{flag}\n"
        else:
            report += "âœ… *ç„¡æ˜é¡¯å•é¡Œ*\n"

        report += "\n---\n\n## ğŸ’¡ æ”¹é€²å»ºè­°\n\n"

        # æ ¹æ“šç‹€æ…‹æä¾›å»ºè­°
        if score.quality_status == 'excellent':
            report += "ğŸ‰ **å…§å®¹å“è³ªå„ªç§€ï¼** å®Œå…¨ç¬¦åˆ Google Helpful Content æ¨™æº–ã€‚\n\n"
            report += "âœ… é€™æ˜¯çœŸæ­£ç‚ºç”¨æˆ¶å‰µä½œçš„æœ‰åƒ¹å€¼å…§å®¹ã€‚\n\n"
            report += "**ä¿æŒå„ªå‹¢**ï¼š\n"
            report += "- ç¹¼çºŒæä¾›ç¬¬ä¸€æ‰‹ç¶“é©—å’Œå¯¦ç”¨å»ºè­°\n"
            report += "- ä¿æŒå…§å®¹çš„æ·±åº¦å’Œå¯æ“ä½œæ€§\n"
            report += "- å®šæœŸæ›´æ–°ä»¥ä¿æŒæ™‚æ•ˆæ€§\n\n"

        elif score.quality_status == 'good':
            report += "ğŸ‘ **å…§å®¹å“è³ªè‰¯å¥½**ï¼Œæ¥è¿‘å„ªç§€æ¨™æº–ï¼š\n\n"
            report += "**å¿«é€Ÿæå‡å»ºè­°**ï¼š\n"

            if score.people_first_score < 80:
                report += "- æ¸›å°‘ SEO å°å‘çš„è©å½™ï¼Œä½¿ç”¨æ›´è‡ªç„¶çš„ç”¨æˆ¶èªè¨€\n"

            if score.authenticity_score < 80:
                report += "- åŠ å…¥æ›´å¤šå€‹äººç¶“é©—å’ŒçœŸå¯¦æ¡ˆä¾‹\n"

            if score.value_density_score < 80:
                report += "- å¢åŠ å¯¦è³ªå…§å®¹ï¼ˆæ•¸æ“šã€æ¡ˆä¾‹ã€æ­¥é©Ÿï¼‰\n"

            if score.actionability_score < 80:
                report += "- æä¾›æ›´å…·é«”çš„è¡Œå‹•æ­¥é©Ÿå’Œå·¥å…·\n"

            report += "\n"

        elif score.quality_status == 'acceptable':
            report += "âš ï¸ **å…§å®¹å¯æ¥å—ï¼Œä½†æœ‰æ˜é¡¯æ”¹é€²ç©ºé–“**ï¼š\n\n"
            report += "**å„ªå…ˆæ”¹é€²é …ç›®**ï¼š\n\n"

            # æ‰¾å‡ºåˆ†æ•¸æœ€ä½çš„ç¶­åº¦
            dims = [
                ('People-First', score.people_first_score),
                ('Authenticity', score.authenticity_score),
                ('Value Density', score.value_density_score),
                ('Actionability', score.actionability_score)
            ]
            dims.sort(key=lambda x: x[1])

            lowest_dim = dims[0]
            report += f"1. **{lowest_dim[0]} ({lowest_dim[1]:.1f}åˆ†)** - æœ€éœ€è¦æ”¹é€²\n"

            if lowest_dim[0] == 'People-First':
                report += "   - æª¢æŸ¥æ˜¯å¦éåº¦ SEO å„ªåŒ–\n"
                report += "   - ä½¿ç”¨æ›´è‡ªç„¶çš„èªè¨€\n"
                report += "   - é¿å…é—œéµå­—å †ç Œ\n"

            elif lowest_dim[0] == 'Authenticity':
                report += "   - åŠ å…¥ç¬¬ä¸€æ‰‹ä½¿ç”¨ç¶“é©—\n"
                report += "   - åˆ†äº«å€‹äººè§€é»å’Œç™¼ç¾\n"
                report += "   - æä¾›å¹³è¡¡çš„å„ªç¼ºé»è©•åƒ¹\n"

            elif lowest_dim[0] == 'Value Density':
                report += "   - æ¸›å°‘ç©ºè©±å’Œé™³è©æ¿«èª¿\n"
                report += "   - åŠ å…¥å…·é«”æ•¸æ“šå’Œæ¡ˆä¾‹\n"
                report += "   - å¢åŠ å…§å®¹æ·±åº¦\n"

            elif lowest_dim[0] == 'Actionability':
                report += "   - æä¾›æ¸…æ™°çš„æ­¥é©Ÿèªªæ˜\n"
                report += "   - åŠ å…¥å¯¦ç”¨å·¥å…·å’Œè³‡æº\n"
                report += "   - ä½¿ç”¨è¡Œå‹•å°å‘èªè¨€\n"

            report += "\n"

            if dims[1][1] < 65:
                report += f"2. **{dims[1][0]} ({dims[1][1]:.1f}åˆ†)** - æ¬¡è¦æ”¹é€²\n"

            report += "\n"

        else:  # poor
            report += "ğŸ”´ **å…§å®¹å“è³ªä¸ä½³ï¼Œå¼·çƒˆå»ºè­°é‡å¯«**ï¼š\n\n"
            report += "**åš´é‡å•é¡Œ**ï¼š\n\n"

            if score.people_first_score < 50:
                report += "- ğŸ”´ éåº¦ SEO å„ªåŒ–ï¼Œéœ€è¦æ”¹ç‚ºç”¨æˆ¶å°å‘å¯«ä½œ\n"

            if score.authenticity_score < 50:
                report += "- ğŸ”´ ç¼ºä¹ç¬¬ä¸€æ‰‹ç¶“é©—ï¼Œå…§å®¹ä¸å¤ çœŸå¯¦\n"

            if score.value_density_score < 50:
                report += "- ğŸ”´ å¯¦è³ªå…§å®¹ä¸è¶³ï¼Œç©ºè©±éå¤š\n"

            if score.actionability_score < 50:
                report += "- ğŸ”´ ç¼ºä¹å¯æ“ä½œçš„å»ºè­°ï¼Œå°è®€è€…å¹«åŠ©æœ‰é™\n"

            report += "\n**é‡å¯«æŒ‡å¼•**ï¼š\n\n"
            report += "1. **ç¢ºç«‹ç”¨æˆ¶åƒ¹å€¼**ï¼šé€™ç¯‡æ–‡ç« èƒ½å¹«è®€è€…è§£æ±ºä»€éº¼å•é¡Œï¼Ÿ\n"
            report += "2. **åŠ å…¥çœŸå¯¦ç¶“é©—**ï¼šåˆ†äº«ä½ çš„è¦ªèº«ä½¿ç”¨ç¶“é©—å’Œç™¼ç¾\n"
            report += "3. **æä¾›å¯¦ç”¨å»ºè­°**ï¼šçµ¦å‡ºå…·é«”çš„æ­¥é©Ÿå’Œå¯åŸ·è¡Œçš„æ–¹æ³•\n"
            report += "4. **æ¸›å°‘ SEO ç—•è·¡**ï¼šç”¨è‡ªç„¶èªè¨€å¯«ä½œï¼Œä¸è¦åˆ»æ„å †ç Œé—œéµå­—\n"
            report += "5. **å¢åŠ æ·±åº¦å’Œç´°ç¯€**ï¼šæä¾›æ•¸æ“šã€æ¡ˆä¾‹ã€å°æ¯”\n\n"

        report += "---\n\n## ğŸ“š Google Helpful Content System æ ¸å¿ƒåŸå‰‡\n\n"

        report += """### âœ… People-First Content Checklist

**å…§å®¹å‰µä½œå‰å•è‡ªå·±**ï¼š

1. â“ ä½ çš„å…§å®¹æ˜¯å¦å±•ç¤ºäº†ç¬¬ä¸€æ‰‹ç¶“é©—å’Œæ·±å…¥çŸ¥è­˜ï¼Ÿ
2. â“ ä½ çš„ç¶²ç«™æ˜¯å¦æœ‰æ˜ç¢ºçš„ä¸»é¡Œæˆ–å°ˆæ¥­é ˜åŸŸï¼Ÿ
3. â“ è®€è€…çœ‹å®Œå¾Œï¼Œæ˜¯å¦æœƒè¦ºå¾—å­¸åˆ°äº†æ±è¥¿ï¼Ÿ
4. â“ è®€è€…æ˜¯å¦æœƒå°‡ä½ çš„ç¶²ç«™åŠ å…¥æ›¸ç±¤æˆ–æ¨è–¦çµ¦æœ‹å‹ï¼Ÿ

**é¿å…çš„åšæ³•**ï¼š

1. âŒ ä¸»è¦ç‚ºäº†æœå°‹å¼•æ“æ’åè€Œå‰µä½œå…§å®¹
2. âŒ å¤§é‡ç”¢ç”Ÿå¤šä¸»é¡Œå…§å®¹ï¼Œå¸Œæœ›å…¶ä¸­ä¸€äº›èƒ½æ’å
3. âŒ ä½¿ç”¨ AI å¤§é‡ç”Ÿæˆå…§å®¹ï¼Œä½†æœªç¶“äººå·¥å¯©æŸ¥å’Œç·¨è¼¯
4. âŒ ç¸½çµå…¶ä»–äººçš„å…§å®¹ï¼Œæ²’æœ‰åŠ å…¥å¯¦è³ªåƒ¹å€¼
5. âŒ åªæ˜¯ç‚ºäº†æ¹Šå­—æ•¸è€Œå¯«é•·æ–‡

### ğŸ¯ 2025 å¹´ Helpful Content System è¦é»

- **èˆ‡æ ¸å¿ƒæ›´æ–°æ•´åˆ**ï¼šç¾åœ¨æ˜¯å–®ä¸€ç³»çµ±ï¼Œå½±éŸ¿æ›´å¤§
- **æ¸›å°‘ 45% ä½å“è³ªå…§å®¹**ï¼šGoogle çš„æ˜ç¢ºç›®æ¨™
- **AI å…§å®¹å¯ä»¥ï¼Œä½†éœ€è¦å¯©æŸ¥**ï¼šä¸æ˜¯åå° AIï¼Œè€Œæ˜¯åå°ä½å“è³ª
- **çœŸå¯¦ç¶“é©—æ¯”ç†è«–æ›´é‡è¦**ï¼šE-E-A-T çš„ Experience ç¶­åº¦

"""

        report += f"\n---\n\n*å ±å‘Šç”Ÿæˆæ™‚é–“: {self._get_timestamp()}*\n"

        # å¦‚æœæŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼Œå¯«å…¥æª”æ¡ˆ
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ… Helpful Content å ±å‘Šå·²ç”Ÿæˆ: {output_path}")

        return report

    def _score_to_quality(self, score: float) -> str:
        """åˆ†æ•¸è½‰å“è³ªç­‰ç´š"""
        if score >= 80:
            return 'excellent'
        elif score >= 65:
            return 'good'
        elif score >= 50:
            return 'acceptable'
        else:
            return 'poor'

    def _get_timestamp(self) -> str:
        """å–å¾—ç•¶å‰æ™‚é–“æˆ³è¨˜"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


def main():
    """å‘½ä»¤è¡Œä»‹é¢"""
    parser = argparse.ArgumentParser(
        description='åˆ†ææ–‡ç« æ˜¯å¦ç¬¦åˆ Google Helpful Content æ¨™æº–'
    )
    parser.add_argument(
        'article',
        help='å¾…åˆ†æçš„æ–‡ç« æª”æ¡ˆè·¯å¾‘'
    )
    parser.add_argument(
        '-e', '--experience',
        help='ç¶“é©—æª”æ¡ˆè·¯å¾‘ï¼ˆexperience_profile.mdï¼‰'
    )
    parser.add_argument(
        '-o', '--output',
        help='è¼¸å‡ºå ±å‘Šè·¯å¾‘ï¼ˆå¯é¸ï¼‰'
    )

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = HelpfulContentAnalyzer()

    # åŸ·è¡Œåˆ†æ
    print(f"ğŸ” åˆ†ææ–‡ç« : {args.article}")

    if args.experience:
        print(f"ğŸ“ åƒè€ƒç¶“é©—æª”æ¡ˆ: {args.experience}")

    score = analyzer.analyze(args.article, args.experience)

    # ç”Ÿæˆå ±å‘Š
    report = analyzer.generate_markdown_report(score, args.output)

    # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼Œåˆ—å°åˆ°çµ‚ç«¯
    if not args.output:
        print("\n" + report)

    # é¡¯ç¤ºæ‘˜è¦
    quality_emoji = {'excellent': 'ğŸŸ¢', 'good': 'ğŸŸ¡', 'acceptable': 'ğŸŸ ', 'poor': 'ğŸ”´'}

    print(f"\n{quality_emoji[score.quality_status]} æœ‰åƒ¹å€¼åº¦: {score.overall_score}/100 ({score.quality_status.upper()})")
    print(f"ğŸ“Š ç¶­åº¦åˆ†æ•¸:")
    print(f"  - People-First: {score.people_first_score:.1f}/100")
    print(f"  - Authenticity: {score.authenticity_score:.1f}/100")
    print(f"  - Value Density: {score.value_density_score:.1f}/100")
    print(f"  - Actionability: {score.actionability_score:.1f}/100")


if __name__ == '__main__':
    main()
