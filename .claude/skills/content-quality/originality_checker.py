#!/usr/bin/env python3
"""
Originality Checker - åŸå‰µæ€§æª¢æ¸¬å™¨
é˜²æ­¢ Google SpamBrain æ‡²ç½°

SpamBrain æ˜¯ Google çš„ AI é©…å‹•ååƒåœ¾ç³»çµ±ï¼Œæª¢æ¸¬ï¼š
- æŠ„è¥²/è¤‡è£½å…§å®¹
- ä½å“è³ªæ”¹å¯«
- å…§å®¹è¾²å ´æ¨¡å¼
- éåº¦ç›¸ä¼¼çš„æ–‡ç« 

2025 å¹´ 8 æœˆæ›´æ–°å¾Œï¼ŒSpamBrain æ›´åš´æ ¼

Version: 1.0.0
Date: 2025-11-04
"""

import re
import hashlib
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
from difflib import SequenceMatcher


@dataclass
class OriginalityScore:
    """åŸå‰µæ€§è©•åˆ†çµæœ"""
    overall_score: float  # ç¸½åˆ† (0-100)
    unique_content_ratio: float  # ç¨ç‰¹å…§å®¹æ¯”ä¾‹ (0-1)
    duplicate_phrases: List[str]  # é‡è¤‡çš„ç‰‡æ®µ
    similarity_warnings: List[str]  # ç›¸ä¼¼åº¦è­¦å‘Š
    quality_status: str  # excellent/good/acceptable/poor
    risk_level: str  # low/medium/high


class OriginalityChecker:
    """åŸå‰µæ€§æª¢æ¸¬å™¨"""

    def __init__(self):
        self.min_phrase_length = 10  # æœ€å°æª¢æŸ¥ç‰‡æ®µé•·åº¦ï¼ˆå­—ï¼‰
        self.similarity_threshold = 0.8  # ç›¸ä¼¼åº¦è­¦å‘Šé–¾å€¼
        self.duplicate_threshold = 0.3  # é‡è¤‡å…§å®¹è­¦å‘Šé–¾å€¼

    def check_article(
        self,
        article_path: str,
        reference_articles: Optional[List[str]] = None
    ) -> OriginalityScore:
        """
        æª¢æŸ¥æ–‡ç« åŸå‰µæ€§

        Args:
            article_path: å¾…æª¢æŸ¥æ–‡ç« è·¯å¾‘
            reference_articles: åƒè€ƒæ–‡ç« åˆ—è¡¨ï¼ˆå¯é¸ï¼Œç”¨æ–¼æ¯”å°ï¼‰

        Returns:
            OriginalityScore: åŸå‰µæ€§è©•åˆ†
        """
        # è®€å–æ–‡ç« 
        with open(article_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # æ¸…ç†ä¸¦æå–ç´”æ–‡æœ¬
        clean_content = self._clean_content(content)

        # æª¢æŸ¥ 1: è‡ªæˆ‘é‡è¤‡æª¢æ¸¬
        self_dup_ratio, duplicate_phrases = self._check_self_duplication(clean_content)

        # æª¢æŸ¥ 2: èˆ‡åƒè€ƒæ–‡ç« æ¯”å°ï¼ˆå¦‚æœæä¾›ï¼‰
        similarity_warnings = []
        max_similarity = 0.0

        if reference_articles:
            for ref_path in reference_articles:
                similarity, warnings = self._compare_with_reference(
                    clean_content,
                    ref_path
                )
                max_similarity = max(max_similarity, similarity)
                similarity_warnings.extend(warnings)

        # æª¢æŸ¥ 3: å¸¸è¦‹æŠ„è¥²æ¨¡å¼
        pattern_warnings = self._check_plagiarism_patterns(clean_content)
        similarity_warnings.extend(pattern_warnings)

        # è¨ˆç®—ç¨ç‰¹å…§å®¹æ¯”ä¾‹
        unique_ratio = 1.0 - self_dup_ratio

        # å¦‚æœæœ‰åƒè€ƒæ–‡ç« ï¼Œè€ƒæ…®å¤–éƒ¨ç›¸ä¼¼åº¦
        if reference_articles and max_similarity > 0:
            unique_ratio = min(unique_ratio, 1.0 - max_similarity)

        # è¨ˆç®—ç¸½åˆ† (0-100)
        overall_score = self._calculate_overall_score(
            unique_ratio,
            len(duplicate_phrases),
            len(similarity_warnings)
        )

        # åˆ¤æ–·å“è³ªç‹€æ…‹
        quality_status = self._assess_quality(overall_score)

        # åˆ¤æ–·é¢¨éšªç­‰ç´š
        risk_level = self._assess_risk(overall_score, unique_ratio)

        return OriginalityScore(
            overall_score=overall_score,
            unique_content_ratio=unique_ratio,
            duplicate_phrases=duplicate_phrases,
            similarity_warnings=similarity_warnings,
            quality_status=quality_status,
            risk_level=risk_level
        )

    def _clean_content(self, content: str) -> str:
        """æ¸…ç†å…§å®¹ï¼Œæå–ç´”æ–‡æœ¬"""
        # ç§»é™¤ frontmatter
        content = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)

        # ç§»é™¤ç¨‹å¼ç¢¼å€å¡Šï¼ˆä¸æª¢æŸ¥ç¨‹å¼ç¢¼é‡è¤‡ï¼‰
        content = re.sub(r'```.*?```', '', content, flags=re.DOTALL)

        # ç§»é™¤è¡Œå…§ç¨‹å¼ç¢¼
        content = re.sub(r'`[^`]+`', '', content)

        # ç§»é™¤é€£çµ
        content = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', content)

        # ç§»é™¤åœ–ç‰‡
        content = re.sub(r'!\[([^\]]*)\]\([^\)]+\)', '', content)

        # ç§»é™¤ markdown æ ¼å¼
        content = re.sub(r'\*\*([^*]+)\*\*', r'\1', content)
        content = re.sub(r'\*([^*]+)\*', r'\1', content)

        # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿ
        content = re.sub(r'^#+\s+', '', content, flags=re.MULTILINE)

        # æ­£è¦åŒ–ç©ºç™½
        content = re.sub(r'\s+', ' ', content)

        return content.strip()

    def _check_self_duplication(self, content: str) -> Tuple[float, List[str]]:
        """
        æª¢æŸ¥æ–‡ç« å…§éƒ¨çš„è‡ªæˆ‘é‡è¤‡

        Returns:
            (é‡è¤‡æ¯”ä¾‹, é‡è¤‡ç‰‡æ®µåˆ—è¡¨)
        """
        # åˆ†å‰²æˆå¥å­
        sentences = self._split_into_sentences(content)

        if len(sentences) < 3:
            return 0.0, []

        # å»ºç«‹å¥å­æŒ‡ç´‹
        sentence_hashes = {}
        duplicate_phrases = []

        for sentence in sentences:
            if len(sentence) < self.min_phrase_length:
                continue

            # æ­£è¦åŒ–å¥å­ï¼ˆç§»é™¤æ¨™é»ã€ç©ºç™½ï¼‰
            normalized = re.sub(r'[^\w]', '', sentence.lower())

            # è¨ˆç®—æŒ‡ç´‹
            fingerprint = hashlib.md5(normalized.encode()).hexdigest()[:16]

            if fingerprint in sentence_hashes:
                # ç™¼ç¾é‡è¤‡
                if sentence not in duplicate_phrases:
                    duplicate_phrases.append(sentence[:100] + "..." if len(sentence) > 100 else sentence)
            else:
                sentence_hashes[fingerprint] = sentence

        # è¨ˆç®—é‡è¤‡æ¯”ä¾‹
        duplicate_ratio = len(duplicate_phrases) / len(sentences) if sentences else 0.0

        return duplicate_ratio, duplicate_phrases

    def _split_into_sentences(self, content: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ç‚ºå¥å­"""
        # ä»¥å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', content)

        # éæ¿¾ç©ºå¥å­å’Œå¤ªçŸ­çš„å¥å­
        sentences = [s.strip() for s in sentences if len(s.strip()) >= self.min_phrase_length]

        return sentences

    def _compare_with_reference(
        self,
        content: str,
        reference_path: str
    ) -> Tuple[float, List[str]]:
        """
        èˆ‡åƒè€ƒæ–‡ç« æ¯”å°ç›¸ä¼¼åº¦

        Returns:
            (ç›¸ä¼¼åº¦åˆ†æ•¸ 0-1, è­¦å‘Šåˆ—è¡¨)
        """
        # è®€å–åƒè€ƒæ–‡ç« 
        try:
            with open(reference_path, 'r', encoding='utf-8') as f:
                ref_content = f.read()
        except Exception as e:
            return 0.0, [f"ç„¡æ³•è®€å–åƒè€ƒæ–‡ç«  {reference_path}: {e}"]

        # æ¸…ç†åƒè€ƒå…§å®¹
        ref_clean = self._clean_content(ref_content)

        # è¨ˆç®—æ•´é«”ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, content, ref_clean).ratio()

        warnings = []

        if similarity > self.similarity_threshold:
            warnings.append(
                f"âš ï¸ èˆ‡ {Path(reference_path).name} ç›¸ä¼¼åº¦éé«˜: {similarity:.1%}"
            )

        # æª¢æŸ¥å…±åŒç‰‡æ®µ
        common_phrases = self._find_common_phrases(content, ref_clean)

        if len(common_phrases) > 3:
            warnings.append(
                f"âš ï¸ èˆ‡ {Path(reference_path).name} æœ‰ {len(common_phrases)} å€‹ç›¸åŒç‰‡æ®µ"
            )

        return similarity, warnings

    def _find_common_phrases(self, text1: str, text2: str) -> List[str]:
        """æ‰¾å‡ºå…©å€‹æ–‡æœ¬çš„å…±åŒç‰‡æ®µ"""
        sentences1 = set(self._split_into_sentences(text1))
        sentences2 = set(self._split_into_sentences(text2))

        # æ‰¾å‡ºå®Œå…¨ç›¸åŒçš„å¥å­
        common = sentences1 & sentences2

        # éæ¿¾å¤ªçŸ­çš„
        common = [s for s in common if len(s) >= self.min_phrase_length]

        return list(common)

    def _check_plagiarism_patterns(self, content: str) -> List[str]:
        """
        æª¢æŸ¥å¸¸è¦‹çš„æŠ„è¥²æ¨¡å¼

        SpamBrain æœƒæª¢æ¸¬çš„æ¨¡å¼ï¼š
        - éåº¦å¼•ç”¨è€Œç„¡åŸå‰µè©•è«–
        - åˆ—è¡¨å¼å…§å®¹ï¼ˆæ²’æœ‰æ·±åº¦ï¼‰
        - é‡è¤‡çš„é–‹é ­/çµå°¾
        """
        warnings = []

        # æ¨¡å¼ 1: éå¤šã€Œæ ¹æ“š...ã€ã€Œæ“š...ã€ï¼ˆå¯èƒ½æ˜¯éåº¦å¼•ç”¨ï¼‰
        quote_count = len(re.findall(r'æ ¹æ“š|æ“š[^èªª]|å¼•ç”¨|ä¾†æº', content))
        sentences_count = len(self._split_into_sentences(content))

        if sentences_count > 0 and quote_count / sentences_count > 0.2:
            warnings.append(
                f"âš ï¸ éåº¦å¼•ç”¨ï¼š{quote_count} è™•å¼•ç”¨ / {sentences_count} å¥ "
                f"({quote_count/sentences_count:.1%}ï¼Œå»ºè­° < 20%)"
            )

        # æ¨¡å¼ 2: éå¤šåˆ—è¡¨è€Œç„¡æ®µè½èªªæ˜ï¼ˆå…§å®¹è¾²å ´æ¨¡å¼ï¼‰
        list_items = len(re.findall(r'^\s*[-*]\s+', content, re.MULTILINE))
        paragraphs = len(re.split(r'\n\n+', content))

        if paragraphs > 0 and list_items / paragraphs > 2:
            warnings.append(
                f"âš ï¸ éåº¦åˆ—è¡¨åŒ–ï¼š{list_items} å€‹åˆ—è¡¨é … / {paragraphs} æ®µ "
                f"ï¼ˆå»ºè­°åŠ å…¥æ›´å¤šæ®µè½èªªæ˜ï¼‰"
            )

        # æ¨¡å¼ 3: é‡è¤‡çš„å¥å¼çµæ§‹ï¼ˆAIç”Ÿæˆå…§å®¹ç‰¹å¾µï¼‰
        sentence_starts = [s[:20] for s in self._split_into_sentences(content) if len(s) >= 20]
        if len(sentence_starts) > 10:
            # æª¢æŸ¥é–‹é ­é‡è¤‡
            start_patterns = {}
            for start in sentence_starts:
                # å–å‰3å­—ä½œç‚ºæ¨¡å¼
                pattern = start[:6]
                start_patterns[pattern] = start_patterns.get(pattern, 0) + 1

            max_repeat = max(start_patterns.values())
            if max_repeat > len(sentence_starts) * 0.3:
                warnings.append(
                    f"âš ï¸ å¥å¼çµæ§‹éæ–¼é‡è¤‡ï¼šæŸç¨®é–‹é ­å‡ºç¾ {max_repeat} æ¬¡ "
                    f"ï¼ˆå»ºè­°å¤šæ¨£åŒ–å¥å¼ï¼‰"
                )

        return warnings

    def _calculate_overall_score(
        self,
        unique_ratio: float,
        duplicate_count: int,
        warning_count: int
    ) -> float:
        """
        è¨ˆç®—ç¸½é«”åŸå‰µæ€§åˆ†æ•¸

        ç®—æ³•ï¼š
        - åŸºç¤åˆ† = unique_ratio * 100
        - æ¯å€‹é‡è¤‡ç‰‡æ®µ -5 åˆ†
        - æ¯å€‹è­¦å‘Š -3 åˆ†
        """
        base_score = unique_ratio * 100

        # æ‰£åˆ†
        penalty = duplicate_count * 5 + warning_count * 3

        overall = max(0, min(100, base_score - penalty))

        return round(overall, 1)

    def _assess_quality(self, score: float) -> str:
        """è©•ä¼°å“è³ªç­‰ç´š"""
        if score >= 85:
            return 'excellent'
        elif score >= 70:
            return 'good'
        elif score >= 50:
            return 'acceptable'
        else:
            return 'poor'

    def _assess_risk(self, score: float, unique_ratio: float) -> str:
        """è©•ä¼° SpamBrain é¢¨éšªç­‰ç´š"""
        if score >= 80 and unique_ratio >= 0.9:
            return 'low'
        elif score >= 60 and unique_ratio >= 0.7:
            return 'medium'
        else:
            return 'high'

    def generate_markdown_report(
        self,
        score: OriginalityScore,
        output_path: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„åŸå‰µæ€§å ±å‘Š

        Args:
            score: åŸå‰µæ€§è©•åˆ†
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰

        Returns:
            str: Markdown æ ¼å¼å ±å‘Š
        """
        # ç‹€æ…‹ emoji
        quality_emoji = {
            'excellent': 'ğŸŸ¢',
            'good': 'ğŸŸ¡',
            'acceptable': 'ğŸŸ ',
            'poor': 'ğŸ”´'
        }

        risk_emoji = {
            'low': 'âœ…',
            'medium': 'âš ï¸',
            'high': 'ğŸ”´'
        }

        report = f"""# åŸå‰µæ€§æª¢æ¸¬å ±å‘Š

## ğŸ“Š ç¸½é«”è©•åˆ†

**åŸå‰µæ€§åˆ†æ•¸**: {score.overall_score}/100 {quality_emoji[score.quality_status]}

**å“è³ªç­‰ç´š**: {score.quality_status.upper()}

**SpamBrain é¢¨éšª**: {risk_emoji[score.risk_level]} {score.risk_level.upper()}

---

## ğŸ“ˆ è©³ç´°æŒ‡æ¨™

### ç¨ç‰¹å…§å®¹æ¯”ä¾‹
**{score.unique_content_ratio:.1%}** ({quality_emoji[score.quality_status]})

- ğŸŸ¢ å„ªç§€: >= 90%
- ğŸŸ¡ è‰¯å¥½: 70-89%
- ğŸŸ  å¯æ¥å—: 50-69%
- ğŸ”´ ä¸ä½³: < 50%

### é‡è¤‡ç‰‡æ®µæ•¸é‡
**{len(score.duplicate_phrases)} å€‹**

### ç›¸ä¼¼åº¦è­¦å‘Š
**{len(score.similarity_warnings)} å€‹**

---

## âš ï¸ æª¢æ¸¬åˆ°çš„å•é¡Œ

"""

        # åˆ—å‡ºé‡è¤‡ç‰‡æ®µ
        if score.duplicate_phrases:
            report += "### å…§éƒ¨é‡è¤‡ç‰‡æ®µ\n\n"
            for i, phrase in enumerate(score.duplicate_phrases[:10], 1):
                report += f"{i}. {phrase}\n"

            if len(score.duplicate_phrases) > 10:
                report += f"\n*é‚„æœ‰ {len(score.duplicate_phrases) - 10} å€‹é‡è¤‡ç‰‡æ®µ...*\n"

            report += "\n"
        else:
            report += "### âœ… ç„¡å…§éƒ¨é‡è¤‡\n\n"

        # åˆ—å‡ºç›¸ä¼¼åº¦è­¦å‘Š
        if score.similarity_warnings:
            report += "### ç›¸ä¼¼åº¦è­¦å‘Š\n\n"
            for warning in score.similarity_warnings:
                report += f"- {warning}\n"
            report += "\n"
        else:
            report += "### âœ… ç„¡å¤–éƒ¨ç›¸ä¼¼åº¦å•é¡Œ\n\n"

        report += "---\n\n## ğŸ’¡ æ”¹é€²å»ºè­°\n\n"

        # æ ¹æ“šç‹€æ…‹æä¾›å»ºè­°
        if score.quality_status == 'excellent':
            report += "ğŸ‰ **åŸå‰µæ€§å„ªç§€ï¼** å…§å®¹å®Œå…¨ç¬¦åˆåŸå‰µæ€§æ¨™æº–ã€‚\n\n"
            report += "âœ… å¯ä»¥å®‰å…¨ç™¼å¸ƒï¼Œç„¡ SpamBrain é¢¨éšªã€‚\n\n"

        elif score.quality_status == 'good':
            report += "ğŸ‘ **åŸå‰µæ€§è‰¯å¥½**ï¼Œä½†æœ‰æ”¹é€²ç©ºé–“ï¼š\n\n"

            if len(score.duplicate_phrases) > 0:
                report += "- æ¸›å°‘æ–‡ç« å…§çš„è‡ªæˆ‘é‡è¤‡ï¼Œæ”¹å¯«é‡è¤‡ç‰‡æ®µ\n"

            if len(score.similarity_warnings) > 0:
                report += "- æª¢æŸ¥èˆ‡åƒè€ƒæ–‡ç« çš„ç›¸ä¼¼å…§å®¹ï¼Œå¢åŠ å€‹äººè§€é»\n"

            report += "\nâœ… å¯ä»¥ç™¼å¸ƒï¼ŒSpamBrain é¢¨éšªè¼ƒä½ã€‚\n\n"

        elif score.quality_status == 'acceptable':
            report += "âš ï¸ **åŸå‰µæ€§å¯æ¥å—ï¼Œä½†å»ºè­°æ”¹é€²**ï¼š\n\n"

            report += "**å„ªå…ˆæ”¹é€²é …ç›®**ï¼š\n"

            if score.unique_content_ratio < 0.7:
                report += "- ğŸ”´ **é‡é»**ï¼šå¢åŠ ç¨ç‰¹å…§å®¹ï¼Œæ¸›å°‘é‡è¤‡\n"

            if len(score.duplicate_phrases) > 5:
                report += "- âš ï¸ é‡å¯«é‡è¤‡ç‰‡æ®µï¼Œä½¿ç”¨ä¸åŒçš„è¡¨é”æ–¹å¼\n"

            if len(score.similarity_warnings) > 2:
                report += "- âš ï¸ å¢åŠ åŸå‰µè¦‹è§£ï¼Œæ¸›å°‘å°åƒè€ƒè³‡æ–™çš„ä¾è³´\n"

            report += "\nâš ï¸ å¯ä»¥ç™¼å¸ƒï¼Œä½†å­˜åœ¨ SpamBrain é¢¨éšªï¼Œå»ºè­°å„ªåŒ–ã€‚\n\n"

        else:  # poor
            report += "ğŸ”´ **åŸå‰µæ€§ä¸è¶³ï¼Œå¼·çƒˆå»ºè­°ä¿®æ”¹å¾Œå†ç™¼å¸ƒ**ï¼š\n\n"

            report += "**åš´é‡å•é¡Œ**ï¼š\n"

            if score.unique_content_ratio < 0.5:
                report += "- ğŸ”´ ç¨ç‰¹å…§å®¹æ¯”ä¾‹éä½ ({:.1%})ï¼Œéœ€è¦å¤§å¹…æ”¹å¯«\n".format(score.unique_content_ratio)

            if len(score.duplicate_phrases) > 10:
                report += "- ğŸ”´ éå¤šé‡è¤‡ç‰‡æ®µ ({}å€‹)ï¼Œéœ€è¦æ¸›å°‘è‡ªæˆ‘é‡è¤‡\n".format(len(score.duplicate_phrases))

            if len(score.similarity_warnings) > 5:
                report += "- ğŸ”´ èˆ‡åƒè€ƒæ–‡ç« éæ–¼ç›¸ä¼¼ï¼Œéœ€è¦åŠ å…¥åŸå‰µå…§å®¹\n"

            report += "\nğŸ”´ **é«˜ SpamBrain é¢¨éšª**ï¼šä¸å»ºè­°ç«‹å³ç™¼å¸ƒï¼Œè«‹å…ˆä¿®æ”¹ã€‚\n\n"

            report += "### ä¿®æ”¹å»ºè­°\n\n"
            report += "1. **å¢åŠ åŸå‰µè§€é»**ï¼šåŠ å…¥å€‹äººç¶“é©—ã€æ¡ˆä¾‹ã€åˆ†æ\n"
            report += "2. **æ”¹å¯«é‡è¤‡å…§å®¹**ï¼šç”¨ä¸åŒæ–¹å¼è¡¨é”ç›¸åŒæ¦‚å¿µ\n"
            report += "3. **æ¸›å°‘å¼•ç”¨æ¯”ä¾‹**ï¼šç¢ºä¿åŸå‰µå…§å®¹ > å¼•ç”¨å…§å®¹\n"
            report += "4. **åŠ å…¥å¯¦ä¾‹å’Œç´°ç¯€**ï¼šå…·é«”çš„æ¡ˆä¾‹å’Œæ•¸æ“š\n"
            report += "5. **æª¢æŸ¥ E-E-A-T**ï¼šç¢ºä¿æœ‰ç¬¬ä¸€æ‰‹ç¶“é©—å’Œå°ˆæ¥­è¦‹è§£\n\n"

        report += "---\n\n## ğŸ›¡ï¸ SpamBrain é˜²è­·å»ºè­°\n\n"

        report += """### Google SpamBrain æœƒæª¢æ¸¬ï¼š

1. **æŠ„è¥²å…§å®¹**
   - ç›´æ¥è¤‡è£½ä»–äººæ–‡ç« 
   - ä½å“è³ªæ”¹å¯«ï¼ˆåƒ…æ”¹å‹•å°‘æ•¸è©å½™ï¼‰
   - å…§å®¹è¾²å ´å¼çš„æ–‡ç« æ‹¼æ¹Š

2. **éåº¦ç›¸ä¼¼**
   - èˆ‡è‡ªå·±å…¶ä»–æ–‡ç« éåº¦é‡è¤‡
   - èˆ‡ç¶²è·¯ä¸Šå·²æœ‰å…§å®¹é«˜åº¦ç›¸ä¼¼
   - ä½¿ç”¨ç›¸åŒæ¨¡æ¿ç”Ÿæˆå¤§é‡ç›¸ä¼¼æ–‡ç« 

3. **ä½å“è³ªæ¨¡å¼**
   - ç´”åˆ—è¡¨å¼å…§å®¹ï¼Œç„¡æ·±åº¦åˆ†æ
   - éåº¦å¼•ç”¨ï¼Œç¼ºä¹åŸå‰µè§€é»
   - AI ç”Ÿæˆä½†æœªç¶“äººå·¥ç·¨è¼¯

### âœ… é˜²è­·ç­–ç•¥ï¼š

- **ç¢ºä¿ç¨ç‰¹å…§å®¹ >= 80%**
- **åŠ å…¥ç¬¬ä¸€æ‰‹ç¶“é©—** (E-E-A-T çš„ Experience)
- **æä¾›åŸå‰µè¦‹è§£å’Œåˆ†æ**
- **é¿å…å¤§é‡ç”¢ç”Ÿç›¸ä¼¼æ–‡ç« **
- **å®šæœŸæª¢æŸ¥åŸå‰µæ€§**

"""

        report += f"\n---\n\n*å ±å‘Šç”Ÿæˆæ™‚é–“: {self._get_timestamp()}*\n"

        # å¦‚æœæŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼Œå¯«å…¥æª”æ¡ˆ
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ… åŸå‰µæ€§å ±å‘Šå·²ç”Ÿæˆ: {output_path}")

        return report

    def _get_timestamp(self) -> str:
        """å–å¾—ç•¶å‰æ™‚é–“æˆ³è¨˜"""
        from datetime import datetime
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')


def main():
    """å‘½ä»¤è¡Œä»‹é¢"""
    parser = argparse.ArgumentParser(
        description='æª¢æŸ¥æ–‡ç« åŸå‰µæ€§ï¼Œé˜²æ­¢ SpamBrain æ‡²ç½°'
    )
    parser.add_argument(
        'article',
        help='å¾…æª¢æŸ¥çš„æ–‡ç« æª”æ¡ˆè·¯å¾‘'
    )
    parser.add_argument(
        '-r', '--reference',
        nargs='*',
        help='åƒè€ƒæ–‡ç« åˆ—è¡¨ï¼ˆç”¨æ–¼æ¯”å°ï¼‰'
    )
    parser.add_argument(
        '-o', '--output',
        help='è¼¸å‡ºå ±å‘Šè·¯å¾‘ï¼ˆå¯é¸ï¼‰'
    )

    args = parser.parse_args()

    # å‰µå»ºæª¢æ¸¬å™¨
    checker = OriginalityChecker()

    # åŸ·è¡Œæª¢æ¸¬
    print(f"ğŸ” æª¢æŸ¥æ–‡ç« : {args.article}")

    if args.reference:
        print(f"ğŸ“š åƒè€ƒæ–‡ç« : {len(args.reference)} ç¯‡")

    score = checker.check_article(args.article, args.reference)

    # ç”Ÿæˆå ±å‘Š
    report = checker.generate_markdown_report(score, args.output)

    # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºè·¯å¾‘ï¼Œåˆ—å°åˆ°çµ‚ç«¯
    if not args.output:
        print("\n" + report)

    # é¡¯ç¤ºæ‘˜è¦
    quality_emoji = {'excellent': 'ğŸŸ¢', 'good': 'ğŸŸ¡', 'acceptable': 'ğŸŸ ', 'poor': 'ğŸ”´'}
    risk_emoji = {'low': 'âœ…', 'medium': 'âš ï¸', 'high': 'ğŸ”´'}

    print(f"\n{quality_emoji[score.quality_status]} åŸå‰µæ€§: {score.overall_score}/100 ({score.quality_status.upper()})")
    print(f"{risk_emoji[score.risk_level]} SpamBrain é¢¨éšª: {score.risk_level.upper()}")
    print(f"ğŸ“Š ç¨ç‰¹å…§å®¹æ¯”ä¾‹: {score.unique_content_ratio:.1%}")


if __name__ == '__main__':
    main()
