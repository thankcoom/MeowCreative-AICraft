#!/usr/bin/env python3
"""
Quality Prediction Skill
é æ¸¬æ–‡ç« å“è³ªåˆ†æ•¸å’Œè©•ä¼°é¢¨éšª
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple


class QualityPredictor:
    """å“è³ªé æ¸¬å™¨"""

    def __init__(self):
        self.weights = {
            'eeat': 0.30,
            'seo': 0.25,
            'persuasion': 0.25,
            'engagement': 0.20,
        }

    def extract_features(self, content: str) -> Dict:
        """æå–æ–‡æœ¬ç‰¹å¾µ"""
        lines = content.split('\n')
        words = content.split()
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        paragraphs = [p for p in content.split('\n\n') if p.strip()]

        # æ¨™é¡Œçµ±è¨ˆ
        h1_count = len(re.findall(r'^# [^#]', content, re.MULTILINE))
        h2_count = len(re.findall(r'^## [^#]', content, re.MULTILINE))
        h3_count = len(re.findall(r'^### [^#]', content, re.MULTILINE))

        # åˆ—è¡¨çµ±è¨ˆ
        list_items = len(re.findall(r'^[-*\d]+[.)] ', content, re.MULTILINE))

        # ç¬¬ä¸€äººç¨±
        first_person = len(re.findall(r'æˆ‘|æˆ‘å€‘|æˆ‘çš„', content))

        # å•å¥
        questions = len(re.findall(r'[ï¼Ÿ?]', content))

        # æ•¸æ“šé»
        data_points = len(re.findall(r'\d+[%ï¼…]|\d+\s*(å€‹|æ¬¡|å¹´|å¤©|å…ƒ|è¬|å„„)', content))

        # å¼•ç”¨ä¾†æº
        citations = len(re.findall(r'\[.*?\]\(.*?\)|æ ¹æ“š|ç ”ç©¶é¡¯ç¤º|å ±å‘ŠæŒ‡å‡º', content))

        # CTA æª¢æ¸¬
        cta_patterns = r'é»æ“Š|ç«‹å³|é¦¬ä¸Š|ç¾åœ¨|å…è²»|äº†è§£æ›´å¤š|é–‹å§‹|è©¦è©¦|åŠ å…¥|è¨‚é–±'
        cta_count = len(re.findall(cta_patterns, content))

        # å¿ƒç†è§¸ç™¼è©
        trigger_words = r'é™æ™‚|ç¨å®¶|åƒ…é™|æœ€å¾Œ|ç«‹å³|ç·Šæ€¥|å…è²»|ä¿è­‰|è­‰æ˜|å°ˆå®¶'
        trigger_count = len(re.findall(trigger_words, content))

        return {
            'word_count': len(words),
            'sentence_count': len(sentences),
            'paragraph_count': len(paragraphs),
            'avg_sentence_length': len(words) / max(len(sentences), 1),
            'avg_paragraph_length': len(words) / max(len(paragraphs), 1),
            'h1_count': h1_count,
            'h2_count': h2_count,
            'h3_count': h3_count,
            'total_headings': h1_count + h2_count + h3_count,
            'list_items': list_items,
            'first_person_usage': first_person,
            'question_count': questions,
            'data_points': data_points,
            'citations': citations,
            'cta_count': cta_count,
            'trigger_words': trigger_count,
        }

    def predict_eeat(self, features: Dict) -> Dict:
        """é æ¸¬ E-E-A-T åˆ†æ•¸"""
        # Experience - åŸºæ–¼ç¬¬ä¸€äººç¨±å’Œå…·é«”ç´°ç¯€
        experience = min(100, 50 + features['first_person_usage'] * 3 + features['data_points'] * 2)

        # Expertise - åŸºæ–¼æ·±åº¦å’Œçµæ§‹
        expertise = min(100, 50 + features['total_headings'] * 5 + features['word_count'] // 100)

        # Authoritativeness - åŸºæ–¼å¼•ç”¨å’Œæ•¸æ“š
        authority = min(100, 40 + features['citations'] * 8 + features['data_points'] * 3)

        # Trustworthiness - ç¶œåˆè€ƒé‡
        trust = min(100, 50 + features['citations'] * 5 + features['question_count'] * 2)

        overall = (experience + expertise + authority + trust) / 4

        return {
            'score': int(overall),
            'confidence': 0.85,
            'components': {
                'experience': int(experience),
                'expertise': int(expertise),
                'authoritativeness': int(authority),
                'trustworthiness': int(trust),
            }
        }

    def predict_seo(self, features: Dict) -> Dict:
        """é æ¸¬ SEO åˆ†æ•¸"""
        score = 50

        # æ¨™é¡Œçµæ§‹
        if features['h1_count'] == 1:
            score += 10
        if features['h2_count'] >= 3:
            score += 10
        if features['h3_count'] >= 2:
            score += 5

        # å…§å®¹é•·åº¦
        if features['word_count'] >= 1500:
            score += 10
        elif features['word_count'] >= 1000:
            score += 5

        # åˆ—è¡¨ä½¿ç”¨
        if features['list_items'] >= 3:
            score += 5

        # æ®µè½é•·åº¦é©ä¸­
        if 50 <= features['avg_paragraph_length'] <= 150:
            score += 5

        # å¥å­é•·åº¦é©ä¸­
        if 10 <= features['avg_sentence_length'] <= 25:
            score += 5

        return {
            'score': min(100, score),
            'confidence': 0.88,
        }

    def predict_persuasion(self, features: Dict) -> Dict:
        """é æ¸¬èªªæœåŠ›åˆ†æ•¸"""
        score = 40

        # CTA
        if features['cta_count'] >= 3:
            score += 20
        elif features['cta_count'] >= 2:
            score += 15
        elif features['cta_count'] >= 1:
            score += 10

        # å¿ƒç†è§¸ç™¼
        if features['trigger_words'] >= 5:
            score += 20
        elif features['trigger_words'] >= 3:
            score += 15
        elif features['trigger_words'] >= 1:
            score += 10

        # å•å¥ï¼ˆå¼•ç™¼æ€è€ƒï¼‰
        if features['question_count'] >= 3:
            score += 10
        elif features['question_count'] >= 1:
            score += 5

        # æ•¸æ“šæ”¯æŒ
        if features['data_points'] >= 3:
            score += 10

        return {
            'score': min(100, score),
            'confidence': 0.82,
        }

    def predict_engagement(self, features: Dict) -> Dict:
        """é æ¸¬åƒèˆ‡åº¦åˆ†æ•¸"""
        score = 50

        # é–‹é ­å•å¥ (å‡è¨­å•å¥åœ¨å‰é¢è¡¨ç¤ºå¥½çš„é–‹é ­)
        if features['question_count'] >= 1:
            score += 10

        # ç¬¬ä¸€äººç¨±ï¼ˆé€£çµæ„Ÿï¼‰
        if features['first_person_usage'] >= 5:
            score += 10
        elif features['first_person_usage'] >= 2:
            score += 5

        # çµæ§‹æ¸…æ™°
        if features['total_headings'] >= 5:
            score += 10

        # åˆ—è¡¨ï¼ˆæ˜“è®€æ€§ï¼‰
        if features['list_items'] >= 5:
            score += 10
        elif features['list_items'] >= 2:
            score += 5

        # é©ç•¶é•·åº¦
        if 1500 <= features['word_count'] <= 3000:
            score += 10

        return {
            'score': min(100, score),
            'confidence': 0.80,
        }

    def assess_risks(self, features: Dict) -> List[Dict]:
        """è©•ä¼°é¢¨éšª"""
        risks = []

        # èªªæœåŠ›é¢¨éšª
        if features['cta_count'] < 2 or features['trigger_words'] < 3:
            probability = 0.65 if features['cta_count'] < 1 else 0.45
            risks.append({
                'type': 'persuasion_risk',
                'severity': 'high',
                'probability': probability,
                'description': 'èªªæœåŠ›å…ƒç´ ä¸è¶³',
                'indicators': [
                    f"CTA åƒ…å‡ºç¾ {features['cta_count']} æ¬¡",
                    f"å¿ƒç†è§¸ç™¼è© {features['trigger_words']} å€‹",
                ],
                'recommendation': 'å¼·åŒ–åŸ·è¡Œ Phase 3.8'
            })

        # åƒèˆ‡åº¦é¢¨éšª
        if features['question_count'] < 1 or features['first_person_usage'] < 2:
            risks.append({
                'type': 'engagement_risk',
                'severity': 'medium',
                'probability': 0.45,
                'description': 'é–‹é ­å¸å¼•åŠ›å¯èƒ½ä¸è¶³',
                'indicators': [
                    f"å•å¥æ•¸é‡: {features['question_count']}",
                    f"ç¬¬ä¸€äººç¨±ä½¿ç”¨: {features['first_person_usage']}",
                ],
                'recommendation': 'æ·»åŠ é–‹å ´å•å¥å’Œå€‹äººåŒ–èªæ°£'
            })

        # AI åµæ¸¬é¢¨éšª - åŸºæ–¼è®Šç•°åº¦
        if features['avg_sentence_length'] > 20 and features['first_person_usage'] < 3:
            risks.append({
                'type': 'ai_detection_risk',
                'severity': 'medium',
                'probability': 0.35,
                'description': 'AI ç—•è·¡é¢¨éšª',
                'indicators': [
                    'å¥å­é•·åº¦è¼ƒä¸€è‡´',
                    'å€‹äººåŒ–èªæ°£ä¸è¶³',
                ],
                'recommendation': 'åŸ·è¡Œ Phase 3.7 Humanizer'
            })

        # SEO é¢¨éšª
        if features['h1_count'] != 1 or features['h2_count'] < 3:
            risks.append({
                'type': 'seo_risk',
                'severity': 'medium',
                'probability': 0.40,
                'description': 'SEO çµæ§‹é¢¨éšª',
                'indicators': [
                    f"H1 æ•¸é‡: {features['h1_count']}",
                    f"H2 æ•¸é‡: {features['h2_count']}",
                ],
                'recommendation': 'å„ªåŒ–æ¨™é¡Œçµæ§‹'
            })

        # å¯ä¿¡åº¦é¢¨éšª
        if features['citations'] < 2:
            risks.append({
                'type': 'credibility_risk',
                'severity': 'medium',
                'probability': 0.50,
                'description': 'å¯ä¿¡åº¦æ”¯æ’ä¸è¶³',
                'indicators': [
                    f"å¼•ç”¨ä¾†æº: {features['citations']} è™•",
                ],
                'recommendation': 'å¢åŠ å¼•ç”¨å’Œæ•¸æ“šæ”¯æŒ'
            })

        return risks

    def predict(self, content: str) -> Dict:
        """å®Œæ•´é æ¸¬"""
        features = self.extract_features(content)

        eeat = self.predict_eeat(features)
        seo = self.predict_seo(features)
        persuasion = self.predict_persuasion(features)
        engagement = self.predict_engagement(features)

        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        overall_score = (
            eeat['score'] * self.weights['eeat'] +
            seo['score'] * self.weights['seo'] +
            persuasion['score'] * self.weights['persuasion'] +
            engagement['score'] * self.weights['engagement']
        )

        overall_confidence = (
            eeat['confidence'] * self.weights['eeat'] +
            seo['confidence'] * self.weights['seo'] +
            persuasion['confidence'] * self.weights['persuasion'] +
            engagement['confidence'] * self.weights['engagement']
        )

        # æ±ºå®šè©•ç´š
        if overall_score >= 90:
            grade = 'A+'
        elif overall_score >= 85:
            grade = 'A'
        elif overall_score >= 80:
            grade = 'B+'
        elif overall_score >= 75:
            grade = 'B'
        elif overall_score >= 70:
            grade = 'C+'
        elif overall_score >= 65:
            grade = 'C'
        else:
            grade = 'D'

        # æ±ºå®šåŸ·è¡Œè·¯å¾‘
        if overall_score >= 85:
            recommendation = 'high_quality_path'
        elif overall_score >= 70:
            recommendation = 'standard_path'
        else:
            recommendation = 'improvement_needed'

        # è©•ä¼°é¢¨éšª
        risks = self.assess_risks(features)

        return {
            'timestamp': datetime.now().isoformat(),
            'features': features,
            'predictions': {
                'eeat': eeat,
                'seo': seo,
                'persuasion': persuasion,
                'engagement': engagement,
                'overall': {
                    'score': int(overall_score),
                    'confidence': round(overall_confidence, 2),
                    'grade': grade,
                }
            },
            'risks': risks,
            'execution_recommendation': recommendation,
        }

    def generate_report(self, prediction: Dict, file_path: str) -> str:
        """ç”Ÿæˆé æ¸¬å ±å‘Š"""
        pred = prediction['predictions']
        risks = prediction['risks']

        report = f"""# å“è³ªé æ¸¬å ±å‘Š

**æ–‡ç« **: {file_path}
**é æ¸¬æ™‚é–“**: {prediction['timestamp'][:19]}
**é æ¸¬ä¿¡å¿ƒåº¦**: {pred['overall']['confidence']*100:.0f}%

## åˆ†æ•¸é æ¸¬

| ç¶­åº¦ | é æ¸¬åˆ†æ•¸ | ä¿¡å¿ƒåº¦ | ç‹€æ…‹ |
|------|----------|--------|------|
| E-E-A-T | {pred['eeat']['score']}/100 | {pred['eeat']['confidence']*100:.0f}% | {'âœ…' if pred['eeat']['score'] >= 75 else 'âš ï¸'} |
| SEO | {pred['seo']['score']}/100 | {pred['seo']['confidence']*100:.0f}% | {'âœ…' if pred['seo']['score'] >= 75 else 'âš ï¸'} |
| èªªæœåŠ› | {pred['persuasion']['score']}/100 | {pred['persuasion']['confidence']*100:.0f}% | {'âœ…' if pred['persuasion']['score'] >= 75 else 'âš ï¸'} |
| åƒèˆ‡åº¦ | {pred['engagement']['score']}/100 | {pred['engagement']['confidence']*100:.0f}% | {'âœ…' if pred['engagement']['score'] >= 75 else 'âš ï¸'} |
| **ç¶œåˆ** | **{pred['overall']['score']}/100** | **{pred['overall']['confidence']*100:.0f}%** | **{pred['overall']['grade']}** |

## E-E-A-T ç´°é …

| ç¶­åº¦ | åˆ†æ•¸ |
|------|------|
| Experience (ç¶“é©—) | {pred['eeat']['components']['experience']}/100 |
| Expertise (å°ˆæ¥­) | {pred['eeat']['components']['expertise']}/100 |
| Authoritativeness (æ¬Šå¨) | {pred['eeat']['components']['authoritativeness']}/100 |
| Trustworthiness (å¯ä¿¡) | {pred['eeat']['components']['trustworthiness']}/100 |

## é¢¨éšªè©•ä¼°

"""
        if risks:
            report += "| é¢¨éšªé¡å‹ | åš´é‡åº¦ | æ©Ÿç‡ | ç‹€æ…‹ |\n"
            report += "|----------|--------|------|------|\n"
            for risk in risks:
                status = "âš ï¸ éœ€è™•ç†" if risk['severity'] == 'high' else "âš ï¸ å»ºè­°æ”¹å–„"
                report += f"| {risk['description']} | {risk['severity']} | {risk['probability']*100:.0f}% | {status} |\n"

            report += "\n### é¢¨éšªè©³æƒ…\n\n"
            for risk in risks:
                report += f"**{risk['type'].replace('_', ' ').title()}**\n"
                report += f"- åš´é‡åº¦: {risk['severity']}\n"
                report += f"- æŒ‡æ¨™: {', '.join(risk['indicators'])}\n"
                report += f"- å»ºè­°: {risk['recommendation']}\n\n"
        else:
            report += "âœ… æœªç™¼ç¾é¡¯è‘—é¢¨éšª\n"

        # åŸ·è¡Œå»ºè­°
        rec = prediction['execution_recommendation']
        if rec == 'high_quality_path':
            rec_text = "é«˜å“è³ªè·¯å¾‘ - å¯è·³ééƒ¨åˆ†å¼·åŒ– Phase"
        elif rec == 'standard_path':
            rec_text = "æ¨™æº–è·¯å¾‘ - åŸ·è¡Œå®Œæ•´æµç¨‹"
        else:
            rec_text = "æ”¹é€²è·¯å¾‘ - éœ€è¦é¡å¤–å¼·åŒ–"

        report += f"""
## åŸ·è¡Œå»ºè­°

**æ¨è–¦è·¯å¾‘**: {rec_text}

"""
        if rec == 'high_quality_path':
            report += "- â­ï¸ å¯è·³é Phase 3.9 (å¦‚éæ•…äº‹å‹æ–‡ç« )\n"
            report += "- âœ… æ­£å¸¸åŸ·è¡Œå…¶ä»– Phase\n"
        elif rec == 'standard_path':
            # æ ¹æ“šé¢¨éšªæ±ºå®šé‡é»
            high_risks = [r for r in risks if r['severity'] == 'high']
            if high_risks:
                report += f"- â­ é‡é»åŸ·è¡Œ: {high_risks[0]['recommendation']}\n"
            report += "- âœ… åŸ·è¡Œå®Œæ•´å¼·åŒ–æµç¨‹\n"
        else:
            report += "- âš ï¸ å»ºè­°è¿”å› Writer Agent ä¿®æ”¹\n"
            report += "- æˆ–å¢åŠ é¡å¤–å¼·åŒ– Phase\n"

        report += f"""
---
*é æ¸¬æ¨¡å‹ç‰ˆæœ¬: 1.0.0*
*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        return report

    def suggest_improvements(self, prediction: Dict, target_score: int) -> List[Dict]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        current = prediction['predictions']['overall']['score']
        gap = target_score - current
        suggestions = []

        if gap <= 0:
            return [{
                'type': 'none',
                'description': f'ç•¶å‰é æ¸¬åˆ†æ•¸ ({current}) å·²é”åˆ°æˆ–è¶…éç›®æ¨™ ({target_score})',
                'priority': 'low'
            }]

        pred = prediction['predictions']

        # æ‰¾å‡ºæœ€å¼±çš„ç¶­åº¦
        scores = {
            'eeat': pred['eeat']['score'],
            'seo': pred['seo']['score'],
            'persuasion': pred['persuasion']['score'],
            'engagement': pred['engagement']['score'],
        }

        sorted_scores = sorted(scores.items(), key=lambda x: x[1])

        for dim, score in sorted_scores[:2]:
            if dim == 'persuasion':
                suggestions.append({
                    'type': 'persuasion_enhancement',
                    'description': 'å¼·åŒ–èªªæœåŠ›å…ƒç´ ',
                    'actions': [
                        'å¢åŠ  2-3 å€‹æ˜ç¢ºçš„ CTA',
                        'æ·»åŠ å¿ƒç†è§¸ç™¼è©ï¼ˆé™æ™‚ã€ç¨å®¶ã€å…è²»ç­‰ï¼‰',
                        'å¢åŠ ç¤¾æœƒè­‰æ˜å’Œæ•¸æ“šæ”¯æŒ'
                    ],
                    'expected_gain': 10,
                    'priority': 'high'
                })
            elif dim == 'engagement':
                suggestions.append({
                    'type': 'engagement_enhancement',
                    'description': 'æå‡è®€è€…åƒèˆ‡åº¦',
                    'actions': [
                        'æ·»åŠ é–‹å ´å•å¥å¼•ç™¼å…±é³´',
                        'å¢åŠ ç¬¬ä¸€äººç¨±ä½¿ç”¨ï¼Œå»ºç«‹é€£çµ',
                        'æ·»åŠ äº’å‹•å…ƒç´ æˆ–è¡Œå‹•æ¸…å–®'
                    ],
                    'expected_gain': 8,
                    'priority': 'high'
                })
            elif dim == 'eeat':
                suggestions.append({
                    'type': 'eeat_enhancement',
                    'description': 'æå‡ E-E-A-T åˆ†æ•¸',
                    'actions': [
                        'å¢åŠ å¼•ç”¨ä¾†æºå’Œæ•¸æ“šæ”¯æŒ',
                        'æ·»åŠ æ›´å¤šå€‹äººç¶“é©—æè¿°',
                        'è£œå……å°ˆå®¶è§€é»æˆ–ç ”ç©¶å¼•ç”¨'
                    ],
                    'expected_gain': 8,
                    'priority': 'medium'
                })
            elif dim == 'seo':
                suggestions.append({
                    'type': 'seo_enhancement',
                    'description': 'å„ªåŒ– SEO çµæ§‹',
                    'actions': [
                        'ç¢ºä¿åªæœ‰ä¸€å€‹ H1 æ¨™é¡Œ',
                        'å¢åŠ  H2/H3 å­æ¨™é¡Œçµæ§‹',
                        'å„ªåŒ–æ®µè½é•·åº¦å’Œåˆ—è¡¨ä½¿ç”¨'
                    ],
                    'expected_gain': 7,
                    'priority': 'medium'
                })

        return suggestions


def main():
    parser = argparse.ArgumentParser(description='Quality Prediction Skill')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # score
    score_parser = subparsers.add_parser('score', help='é æ¸¬å“è³ªåˆ†æ•¸')
    score_parser.add_argument('file', help='æ–‡ç« æª”æ¡ˆè·¯å¾‘')
    score_parser.add_argument('--output', help='è¼¸å‡º JSON è·¯å¾‘')

    # risk
    risk_parser = subparsers.add_parser('risk', help='è©•ä¼°é¢¨éšª')
    risk_parser.add_argument('file', help='æ–‡ç« æª”æ¡ˆè·¯å¾‘')
    risk_parser.add_argument('--output', help='è¼¸å‡ºå ±å‘Šè·¯å¾‘')

    # suggest
    suggest_parser = subparsers.add_parser('suggest', help='ç²å–æ”¹é€²å»ºè­°')
    suggest_parser.add_argument('file', help='æ–‡ç« æª”æ¡ˆè·¯å¾‘')
    suggest_parser.add_argument('--target-score', type=int, default=85, help='ç›®æ¨™åˆ†æ•¸')

    # trend
    trend_parser = subparsers.add_parser('trend', help='æ­·å²è¶¨å‹¢åˆ†æ')
    trend_parser.add_argument('--sessions', type=int, default=10, help='åˆ†æçš„ session æ•¸é‡')
    trend_parser.add_argument('--output', help='è¼¸å‡ºå ±å‘Šè·¯å¾‘')

    # full
    full_parser = subparsers.add_parser('full', help='å®Œæ•´é æ¸¬å ±å‘Š')
    full_parser.add_argument('file', help='æ–‡ç« æª”æ¡ˆè·¯å¾‘')
    full_parser.add_argument('--output', help='è¼¸å‡ºå ±å‘Šè·¯å¾‘')

    args = parser.parse_args()
    predictor = QualityPredictor()

    if args.command in ['score', 'risk', 'suggest', 'full']:
        # è®€å–æ–‡ç« 
        file_path = Path(args.file)
        if not file_path.exists():
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.file}")
            sys.exit(1)

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        prediction = predictor.predict(content)

    if args.command == 'score':
        pred = prediction['predictions']
        print(f"å“è³ªé æ¸¬çµæœ ({args.file}):\n")
        print(f"  E-E-A-T:   {pred['eeat']['score']}/100")
        print(f"  SEO:       {pred['seo']['score']}/100")
        print(f"  èªªæœåŠ›:    {pred['persuasion']['score']}/100")
        print(f"  åƒèˆ‡åº¦:    {pred['engagement']['score']}/100")
        print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  ç¶œåˆåˆ†æ•¸:  {pred['overall']['score']}/100 ({pred['overall']['grade']})")
        print(f"  ä¿¡å¿ƒåº¦:    {pred['overall']['confidence']*100:.0f}%")
        print(f"\nåŸ·è¡Œå»ºè­°: {prediction['execution_recommendation']}")

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(prediction, f, indent=2, ensure_ascii=False)
            print(f"\nå·²ä¿å­˜è‡³: {args.output}")

    elif args.command == 'risk':
        risks = prediction['risks']
        print(f"é¢¨éšªè©•ä¼° ({args.file}):\n")

        if risks:
            for risk in risks:
                severity_icon = "ğŸ”´" if risk['severity'] == 'high' else "ğŸŸ¡"
                print(f"{severity_icon} {risk['description']}")
                print(f"   åš´é‡åº¦: {risk['severity']}, æ©Ÿç‡: {risk['probability']*100:.0f}%")
                print(f"   å»ºè­°: {risk['recommendation']}\n")
        else:
            print("âœ… æœªç™¼ç¾é¡¯è‘—é¢¨éšª")

        if args.output:
            report = predictor.generate_report(prediction, args.file)
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nå ±å‘Šå·²ä¿å­˜è‡³: {args.output}")

    elif args.command == 'suggest':
        suggestions = predictor.suggest_improvements(prediction, args.target_score)

        current = prediction['predictions']['overall']['score']
        print(f"æ”¹é€²å»ºè­° (ç•¶å‰: {current}, ç›®æ¨™: {args.target_score}):\n")

        for i, sug in enumerate(suggestions, 1):
            print(f"{i}. {sug['description']} (é æœŸæå‡: +{sug.get('expected_gain', 'N/A')})")
            if 'actions' in sug:
                for action in sug['actions']:
                    print(f"   - {action}")
            print()

    elif args.command == 'trend':
        print(f"æ­·å²è¶¨å‹¢åˆ†æ (æœ€è¿‘ {args.sessions} å€‹ sessions):")
        print("\nâš ï¸ æ­¤åŠŸèƒ½éœ€è¦ç´¯ç©æ­·å²æ•¸æ“šå¾Œæ‰èƒ½æä¾›åˆ†æ")
        print("å»ºè­°: åŸ·è¡Œæ›´å¤š sessions å¾Œå†æŸ¥çœ‹è¶¨å‹¢")

    elif args.command == 'full':
        report = predictor.generate_report(prediction, args.file)

        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ… å®Œæ•´å ±å‘Šå·²ä¿å­˜è‡³: {args.output}")
        else:
            print(report)

    else:
        parser.print_help()


if __name__ == '__main__':
    main()
