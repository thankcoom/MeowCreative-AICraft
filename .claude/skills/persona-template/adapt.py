#!/usr/bin/env python3
"""
Persona Template Skill
ç®¡ç†å’Œæ‡‰ç”¨è®€è€… Persona æ¨¡æ¿ï¼Œæ”¯æ´å…§å®¹é©é…å’Œå¤šç‰ˆæœ¬ç”Ÿæˆ

Usage:
    python3 adapt.py list
    python3 adapt.py show beginner
    python3 adapt.py adapt input.md --persona beginner --output adapted.md
    python3 adapt.py multi-adapt input.md --personas beginner,expert --output-dir versions/
    python3 adapt.py verify original.md adapted.md --persona beginner
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import yaml


# é è¨­ Persona æ¨¡æ¿
DEFAULT_PERSONAS = {
    "beginner": {
        "id": "beginner",
        "name": "æ–°æ‰‹å°ç™½",
        "description": "å‰›æ¥è§¸æ­¤é ˜åŸŸçš„åˆå­¸è€…",
        "icon": "ğŸŒ±",
        "characteristics": {
            "knowledge_level": 1,
            "attention_span": "short",
            "preferred_format": "visual_heavy",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "simple",
                "explain_jargon": True,
            },
            "structure": {
                "paragraph_length": "short",
                "header_frequency": "dense",
                "list_usage": "high",
            },
            "content": {
                "include_basics": True,
                "example_frequency": "high",
            },
            "tone": {
                "formality": 0.3,
                "encouragement": 0.9,
            },
        },
    },
    "intermediate": {
        "id": "intermediate",
        "name": "é€²éšä½¿ç”¨è€…",
        "description": "æœ‰åŸºç¤çŸ¥è­˜ï¼Œå°‹æ±‚æ·±å…¥ç†è§£",
        "icon": "ğŸ“ˆ",
        "characteristics": {
            "knowledge_level": 3,
            "attention_span": "medium",
            "preferred_format": "balanced",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "medium",
                "explain_jargon": "briefly",
            },
            "structure": {
                "paragraph_length": "medium",
                "header_frequency": "normal",
                "list_usage": "medium",
            },
            "content": {
                "skip_basics": True,
                "best_practices": True,
            },
            "tone": {
                "formality": 0.5,
                "directness": 0.7,
            },
        },
    },
    "expert": {
        "id": "expert",
        "name": "å°ˆå®¶è®€è€…",
        "description": "é ˜åŸŸå°ˆå®¶ï¼Œå°‹æ±‚æ–°çŸ¥å’Œæ·±åº¦",
        "icon": "ğŸ“",
        "characteristics": {
            "knowledge_level": 5,
            "attention_span": "long",
            "preferred_format": "text_heavy",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "high",
                "no_explanations": True,
            },
            "structure": {
                "paragraph_length": "long",
                "header_frequency": "sparse",
                "list_usage": "low",
            },
            "content": {
                "deep_dive": True,
                "edge_cases": True,
            },
            "tone": {
                "formality": 0.8,
                "precision": 0.9,
            },
        },
    },
    "decision_maker": {
        "id": "decision_maker",
        "name": "æ±ºç­–è€…",
        "description": "éœ€è¦åšå‡ºæ±ºå®šçš„ç®¡ç†è€…",
        "icon": "ğŸ’¼",
        "characteristics": {
            "knowledge_level": "varies",
            "attention_span": "short",
            "preferred_format": "summary_first",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "medium",
                "business_focus": True,
            },
            "structure": {
                "executive_summary": True,
                "bullet_points": "high",
            },
            "content": {
                "roi_focus": True,
                "recommendations": True,
            },
            "tone": {
                "formality": 0.7,
                "confidence": 0.8,
            },
        },
    },
    "gen_z": {
        "id": "gen_z",
        "name": "Z ä¸–ä»£",
        "description": "1997-2012 å¹´å‡ºç”Ÿçš„å¹´è¼•è®€è€…",
        "icon": "âš¡",
        "characteristics": {
            "knowledge_level": "varies",
            "attention_span": "very_short",
            "preferred_format": "snackable",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "simple",
                "slang_ok": True,
            },
            "structure": {
                "paragraph_length": "very_short",
                "visual_breaks": True,
            },
            "content": {
                "relevance_first": True,
                "social_proof": True,
            },
            "tone": {
                "formality": 0.1,
                "authenticity": 0.9,
            },
        },
    },
    "professional": {
        "id": "professional",
        "name": "å°ˆæ¥­äººå£«",
        "description": "æœ‰å·¥ä½œç¶“é©—çš„è·å ´äººå£«",
        "icon": "ğŸ‘”",
        "characteristics": {
            "knowledge_level": 3,
            "attention_span": "medium",
            "preferred_format": "practical",
        },
        "adaptation": {
            "vocabulary": {
                "complexity": "medium",
                "industry_terms": True,
            },
            "structure": {
                "problem_solution": True,
                "checklists": True,
            },
            "content": {
                "practical_focus": True,
                "tool_recommendations": True,
            },
            "tone": {
                "formality": 0.6,
                "practical": 0.9,
            },
        },
    },
}

# è‡ªå®šç¾© Persona å­˜å„²è·¯å¾‘
CUSTOM_PERSONAS_PATH = Path(__file__).parent / "custom_personas"


class PersonaAdapter:
    """Persona é©é…å™¨"""

    def __init__(self):
        self.personas = self._load_personas()

    def _load_personas(self) -> Dict:
        """è¼‰å…¥æ‰€æœ‰ Persona"""
        personas = DEFAULT_PERSONAS.copy()

        # è¼‰å…¥è‡ªå®šç¾© Persona
        if CUSTOM_PERSONAS_PATH.exists():
            for f in CUSTOM_PERSONAS_PATH.glob("*.yaml"):
                data = yaml.safe_load(f.read_text(encoding="utf-8"))
                if data and "id" in data:
                    personas[data["id"]] = data

        return personas

    def list_personas(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰ Persona"""
        return [
            {
                "id": p["id"],
                "name": p["name"],
                "icon": p.get("icon", "ğŸ‘¤"),
                "description": p["description"],
                "knowledge_level": p["characteristics"].get("knowledge_level", "varies"),
            }
            for p in self.personas.values()
        ]

    def get_persona(self, persona_id: str) -> Optional[Dict]:
        """ç²å–ç‰¹å®š Persona"""
        return self.personas.get(persona_id)

    def adapt_content(self, content: str, persona_id: str) -> Dict:
        """é©é…å…§å®¹"""
        persona = self.get_persona(persona_id)
        if not persona:
            return {"error": f"Persona ä¸å­˜åœ¨: {persona_id}"}

        original_stats = self._analyze_content(content)
        adapted_content = self._apply_adaptation(content, persona)
        adapted_stats = self._analyze_content(adapted_content)

        return {
            "persona": persona_id,
            "persona_name": persona["name"],
            "original_content": content,
            "adapted_content": adapted_content,
            "original_stats": original_stats,
            "adapted_stats": adapted_stats,
            "changes": self._calculate_changes(original_stats, adapted_stats),
        }

    def _analyze_content(self, content: str) -> Dict:
        """åˆ†æå…§å®¹çµ±è¨ˆ"""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        sentences = [s.strip() for s in sentences if s.strip()]

        words = len(content)
        avg_sentence_len = words / max(1, len(sentences))
        avg_paragraph_len = words / max(1, len(paragraphs))

        # çµ±è¨ˆå°ˆæ¥­è¡“èª (ç°¡åŒ–ç‰ˆ)
        jargon_patterns = [
            r'API', r'SDK', r'æ¡†æ¶', r'æ¼”ç®—æ³•', r'è¿­ä»£',
            r'å„ªåŒ–', r'æ•ˆèƒ½', r'æ¶æ§‹', r'æ¨¡çµ„', r'ä»‹é¢',
        ]
        jargon_count = sum(len(re.findall(p, content)) for p in jargon_patterns)

        # çµ±è¨ˆåˆ—è¡¨é …ç›®
        list_items = len(re.findall(r'^[-*]\s', content, re.MULTILINE))

        # çµ±è¨ˆæ¨™é¡Œ
        headers = len(re.findall(r'^#{1,6}\s', content, re.MULTILINE))

        return {
            "word_count": words,
            "paragraph_count": len(paragraphs),
            "sentence_count": len(sentences),
            "avg_sentence_length": round(avg_sentence_len, 1),
            "avg_paragraph_length": round(avg_paragraph_len, 1),
            "jargon_count": jargon_count,
            "list_items": list_items,
            "header_count": headers,
        }

    def _apply_adaptation(self, content: str, persona: Dict) -> str:
        """æ‡‰ç”¨é©é…è¦å‰‡"""
        adapted = content
        adaptation = persona.get("adaptation", {})

        # è©å½™é©é…
        vocab = adaptation.get("vocabulary", {})
        if vocab.get("explain_jargon"):
            adapted = self._add_jargon_explanations(adapted)
        if vocab.get("complexity") == "simple":
            adapted = self._simplify_vocabulary(adapted)

        # çµæ§‹é©é…
        structure = adaptation.get("structure", {})
        if structure.get("paragraph_length") in ["short", "very_short"]:
            adapted = self._shorten_paragraphs(adapted)
        if structure.get("list_usage") == "high":
            adapted = self._enhance_lists(adapted)
        if structure.get("header_frequency") == "dense":
            adapted = self._add_headers(adapted)

        # å…§å®¹é©é…
        content_rules = adaptation.get("content", {})
        if content_rules.get("include_basics"):
            adapted = self._add_basics(adapted)
        if content_rules.get("example_frequency") == "high":
            adapted = self._enhance_examples(adapted)

        return adapted

    def _add_jargon_explanations(self, content: str) -> str:
        """æ·»åŠ å°ˆæ¥­è¡“èªè§£é‡‹"""
        explanations = {
            "API": "APIï¼ˆæ‡‰ç”¨ç¨‹å¼ä»‹é¢ï¼Œè®“ä¸åŒè»Ÿé«”å¯ä»¥äº’ç›¸æºé€šï¼‰",
            "SDK": "SDKï¼ˆè»Ÿé«”é–‹ç™¼å¥—ä»¶ï¼Œå¹«åŠ©é–‹ç™¼è€…æ›´å¿«å»ºç«‹æ‡‰ç”¨ï¼‰",
        }
        for term, explanation in explanations.items():
            # åªæ›¿æ›ç¬¬ä¸€æ¬¡å‡ºç¾
            if term in content and explanation not in content:
                content = content.replace(term, explanation, 1)
        return content

    def _simplify_vocabulary(self, content: str) -> str:
        """ç°¡åŒ–è©å½™"""
        replacements = {
            "å„ªåŒ–": "æ”¹å–„",
            "è¿­ä»£": "æ”¹é€²",
            "å¯¦ç¾": "åšåˆ°",
            "æ¶æ§‹": "çµæ§‹",
            "é…ç½®": "è¨­å®š",
        }
        for old, new in replacements.items():
            content = content.replace(old, new)
        return content

    def _shorten_paragraphs(self, content: str) -> str:
        """ç¸®çŸ­æ®µè½"""
        paragraphs = content.split('\n\n')
        shortened = []
        for para in paragraphs:
            if len(para) > 300:
                # å˜—è©¦åœ¨ä¸­é–“é»æ–·é–‹
                mid = len(para) // 2
                # æ‰¾æœ€è¿‘çš„å¥è™Ÿ
                break_point = para.rfind('ã€‚', 0, mid + 50)
                if break_point > mid - 100:
                    shortened.append(para[:break_point + 1])
                    shortened.append(para[break_point + 1:].strip())
                else:
                    shortened.append(para)
            else:
                shortened.append(para)
        return '\n\n'.join(shortened)

    def _enhance_lists(self, content: str) -> str:
        """å¢å¼·åˆ—è¡¨ä½¿ç”¨"""
        # ç°¡åŒ–å¯¦ç¾ï¼šåœ¨æœ‰ã€Œé¦–å…ˆã€ã€Œå…¶æ¬¡ã€ç­‰è©çš„åœ°æ–¹å»ºè­°ä½¿ç”¨åˆ—è¡¨
        # é€™è£¡åªæ˜¯ç¤ºæ„ï¼Œå¯¦éš›å¯¦ç¾æœƒæ›´è¤‡é›œ
        return content

    def _add_headers(self, content: str) -> str:
        """å¢åŠ æ¨™é¡Œå¯†åº¦"""
        # ç°¡åŒ–å¯¦ç¾
        return content

    def _add_basics(self, content: str) -> str:
        """æ·»åŠ åŸºç¤èªªæ˜"""
        # ç°¡åŒ–å¯¦ç¾
        return content

    def _enhance_examples(self, content: str) -> str:
        """å¢å¼·ç¯„ä¾‹"""
        # ç°¡åŒ–å¯¦ç¾
        return content

    def _calculate_changes(self, original: Dict, adapted: Dict) -> Dict:
        """è¨ˆç®—è®ŠåŒ–"""
        changes = {}
        for key in original:
            orig_val = original[key]
            adapt_val = adapted[key]
            if isinstance(orig_val, (int, float)) and orig_val > 0:
                change_pct = ((adapt_val - orig_val) / orig_val) * 100
                changes[key] = {
                    "original": orig_val,
                    "adapted": adapt_val,
                    "change_percent": round(change_pct, 1),
                }
        return changes

    def verify_adaptation(self, original: str, adapted: str, persona_id: str) -> Dict:
        """é©—è­‰é©é…å“è³ª"""
        persona = self.get_persona(persona_id)
        if not persona:
            return {"error": f"Persona ä¸å­˜åœ¨: {persona_id}"}

        original_stats = self._analyze_content(original)
        adapted_stats = self._analyze_content(adapted)

        # æ ¸å¿ƒè¨Šæ¯ä¿ç•™æª¢æŸ¥ (ç°¡åŒ–ç‰ˆï¼šæ¯”è¼ƒé—œéµè©)
        original_keywords = set(re.findall(r'[\u4e00-\u9fff]{2,4}', original))
        adapted_keywords = set(re.findall(r'[\u4e00-\u9fff]{2,4}', adapted))
        keyword_retention = len(original_keywords & adapted_keywords) / max(1, len(original_keywords))

        # è¨ˆç®—å¯è®€æ€§åˆ†æ•¸ (ç°¡åŒ–ç‰ˆ)
        readability = self._calculate_readability(adapted_stats)

        # é¢¨æ ¼åŒ¹é…åº¦
        style_match = self._check_style_match(adapted_stats, persona)

        # è¨ˆç®—ç¸½åˆ†
        scores = {
            "core_message_retention": round(keyword_retention * 100, 1),
            "readability_score": readability,
            "style_match": style_match,
        }

        total_score = (
            scores["core_message_retention"] * 0.4 +
            scores["readability_score"] * 0.3 +
            scores["style_match"] * 0.3
        )

        return {
            "persona": persona_id,
            "scores": scores,
            "total_score": round(total_score, 1),
            "passed": total_score >= 80 and keyword_retention >= 0.95,
            "issues": self._identify_issues(scores, persona),
        }

    def _calculate_readability(self, stats: Dict) -> float:
        """è¨ˆç®—å¯è®€æ€§åˆ†æ•¸"""
        # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼å¥å­é•·åº¦å’Œæ®µè½é•·åº¦
        sentence_score = max(0, 100 - (stats["avg_sentence_length"] - 15) * 2)
        paragraph_score = max(0, 100 - (stats["avg_paragraph_length"] - 200) * 0.1)
        return round((sentence_score + paragraph_score) / 2, 1)

    def _check_style_match(self, stats: Dict, persona: Dict) -> float:
        """æª¢æŸ¥é¢¨æ ¼åŒ¹é…åº¦"""
        # ç°¡åŒ–ç‰ˆ
        adaptation = persona.get("adaptation", {})
        structure = adaptation.get("structure", {})

        score = 70  # åŸºç¤åˆ†

        # æ®µè½é•·åº¦æª¢æŸ¥
        expected_para_length = structure.get("paragraph_length", "medium")
        if expected_para_length == "short" and stats["avg_paragraph_length"] < 200:
            score += 15
        elif expected_para_length == "medium" and 200 <= stats["avg_paragraph_length"] <= 400:
            score += 15
        elif expected_para_length == "long" and stats["avg_paragraph_length"] > 400:
            score += 15

        # åˆ—è¡¨ä½¿ç”¨æª¢æŸ¥
        list_usage = structure.get("list_usage", "medium")
        if list_usage == "high" and stats["list_items"] > 5:
            score += 15

        return min(100, score)

    def _identify_issues(self, scores: Dict, persona: Dict) -> List[str]:
        """è­˜åˆ¥å•é¡Œ"""
        issues = []
        if scores["core_message_retention"] < 95:
            issues.append("æ ¸å¿ƒè¨Šæ¯ä¿ç•™ä¸è¶³ï¼Œå»ºè­°æª¢æŸ¥é—œéµå…§å®¹")
        if scores["readability_score"] < 70:
            issues.append("å¯è®€æ€§åä½ï¼Œå»ºè­°ç¸®çŸ­å¥å­æˆ–æ®µè½")
        if scores["style_match"] < 70:
            issues.append("é¢¨æ ¼åŒ¹é…åº¦ä¸è¶³ï¼Œå»ºè­°èª¿æ•´çµæ§‹")
        return issues

    def create_persona(self, persona_id: str, name: str, config_file: Path) -> Dict:
        """å‰µå»ºè‡ªå®šç¾© Persona"""
        if not config_file.exists():
            return {"error": f"é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_file}"}

        config = yaml.safe_load(config_file.read_text(encoding="utf-8"))
        config["id"] = persona_id
        config["name"] = name

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        CUSTOM_PERSONAS_PATH.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜
        output_file = CUSTOM_PERSONAS_PATH / f"{persona_id}.yaml"
        output_file.write_text(
            yaml.dump(config, allow_unicode=True, default_flow_style=False),
            encoding="utf-8"
        )

        return {
            "status": "success",
            "persona_id": persona_id,
            "file": str(output_file),
            "message": f"Persona '{name}' å·²å‰µå»º",
        }


def generate_report(result: Dict) -> str:
    """ç”Ÿæˆé©é…å ±å‘Š"""
    lines = [
        "# Persona é©é…å ±å‘Š",
        "",
        f"**ç›®æ¨™ Persona**: {result['persona_name']} ({result['persona']})",
        f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "## é©é…çµ±è¨ˆ",
        "",
        "| æŒ‡æ¨™ | åŸå§‹ | é©é…å¾Œ | è®ŠåŒ– |",
        "|------|------|--------|------|",
    ]

    changes = result.get("changes", {})
    for key, data in changes.items():
        change_str = f"+{data['change_percent']}%" if data['change_percent'] > 0 else f"{data['change_percent']}%"
        lines.append(f"| {key} | {data['original']} | {data['adapted']} | {change_str} |")

    lines.extend([
        "",
        "---",
        "",
        "*è‡ªå‹•ç”Ÿæˆ by Persona Template Skill*",
    ])

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Persona Template Skill - è®€è€…é©é…")
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # list
    subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰ Persona")

    # show
    show_parser = subparsers.add_parser("show", help="é¡¯ç¤º Persona è©³æƒ…")
    show_parser.add_argument("persona_id", help="Persona ID")

    # adapt
    adapt_parser = subparsers.add_parser("adapt", help="é©é…å…§å®¹")
    adapt_parser.add_argument("input", help="è¼¸å…¥æª”æ¡ˆ")
    adapt_parser.add_argument("--persona", required=True, help="ç›®æ¨™ Persona")
    adapt_parser.add_argument("--output", default="adapted.md", help="è¼¸å‡ºæª”æ¡ˆ")

    # multi-adapt
    multi_parser = subparsers.add_parser("multi-adapt", help="å¤šç‰ˆæœ¬ç”Ÿæˆ")
    multi_parser.add_argument("input", help="è¼¸å…¥æª”æ¡ˆ")
    multi_parser.add_argument("--personas", required=True, help="Persona åˆ—è¡¨ (é€—è™Ÿåˆ†éš”)")
    multi_parser.add_argument("--output-dir", default="adapted_versions", help="è¼¸å‡ºç›®éŒ„")

    # verify
    verify_parser = subparsers.add_parser("verify", help="é©—è­‰é©é…å“è³ª")
    verify_parser.add_argument("original", help="åŸå§‹æª”æ¡ˆ")
    verify_parser.add_argument("adapted", help="é©é…å¾Œæª”æ¡ˆ")
    verify_parser.add_argument("--persona", required=True, help="ç›®æ¨™ Persona")

    # create
    create_parser = subparsers.add_parser("create", help="å‰µå»ºè‡ªå®šç¾© Persona")
    create_parser.add_argument("--id", required=True, help="Persona ID")
    create_parser.add_argument("--name", required=True, help="Persona åç¨±")
    create_parser.add_argument("--config", required=True, help="é…ç½®æª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()
    adapter = PersonaAdapter()

    if args.command == "list":
        personas = adapter.list_personas()
        print("å¯ç”¨çš„ Persona æ¨¡æ¿:\n")
        for p in personas:
            print(f"  {p['icon']} {p['id']}: {p['name']}")
            print(f"     {p['description']}")
            print(f"     çŸ¥è­˜ç¨‹åº¦: {p['knowledge_level']}")
            print()

    elif args.command == "show":
        persona = adapter.get_persona(args.persona_id)
        if persona:
            print(f"{persona.get('icon', 'ğŸ‘¤')} {persona['name']} ({persona['id']})")
            print(f"\n{persona['description']}\n")
            print("ç‰¹å¾µ:")
            for key, value in persona.get("characteristics", {}).items():
                print(f"  - {key}: {value}")
            print("\né©é…è¦å‰‡:")
            print(yaml.dump(persona.get("adaptation", {}), allow_unicode=True, default_flow_style=False))
        else:
            print(f"âŒ Persona ä¸å­˜åœ¨: {args.persona_id}")

    elif args.command == "adapt":
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.input}")
            sys.exit(1)

        content = input_path.read_text(encoding="utf-8")
        result = adapter.adapt_content(content, args.persona)

        if "error" in result:
            print(f"âŒ {result['error']}")
            sys.exit(1)

        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(result["adapted_content"], encoding="utf-8")

        print(f"âœ… å·²é©é…åˆ° {result['persona_name']}")
        print(f"   è¼¸å‡º: {output_path}")
        print(f"   å­—æ•¸è®ŠåŒ–: {result['original_stats']['word_count']} â†’ {result['adapted_stats']['word_count']}")

    elif args.command == "multi-adapt":
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.input}")
            sys.exit(1)

        content = input_path.read_text(encoding="utf-8")
        personas = [p.strip() for p in args.personas.split(",")]
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        print(f"æ­£åœ¨ç”Ÿæˆ {len(personas)} å€‹ç‰ˆæœ¬...\n")
        for persona_id in personas:
            result = adapter.adapt_content(content, persona_id)
            if "error" in result:
                print(f"  âŒ {persona_id}: {result['error']}")
                continue

            output_file = output_dir / f"{persona_id}_version.md"
            output_file.write_text(result["adapted_content"], encoding="utf-8")
            print(f"  âœ… {result['persona_name']} â†’ {output_file.name}")

        print(f"\nâœ… å®Œæˆï¼è¼¸å‡ºç›®éŒ„: {output_dir}")

    elif args.command == "verify":
        original_path = Path(args.original)
        adapted_path = Path(args.adapted)

        if not original_path.exists() or not adapted_path.exists():
            print("âŒ æª”æ¡ˆä¸å­˜åœ¨")
            sys.exit(1)

        original = original_path.read_text(encoding="utf-8")
        adapted = adapted_path.read_text(encoding="utf-8")

        result = adapter.verify_adaptation(original, adapted, args.persona)

        if "error" in result:
            print(f"âŒ {result['error']}")
            sys.exit(1)

        print(f"é©é…å“è³ªé©—è­‰ ({args.persona})")
        print(f"\nç¸½åˆ†: {result['total_score']}/100 {'âœ… é€šé' if result['passed'] else 'âŒ æœªé€šé'}")
        print(f"\nå„é …åˆ†æ•¸:")
        for key, score in result["scores"].items():
            print(f"  - {key}: {score}")

        if result["issues"]:
            print("\nå•é¡Œ:")
            for issue in result["issues"]:
                print(f"  âš ï¸ {issue}")

    elif args.command == "create":
        result = adapter.create_persona(
            args.id,
            args.name,
            Path(args.config)
        )
        if "error" in result:
            print(f"âŒ {result['error']}")
            sys.exit(1)
        print(f"âœ… {result['message']}")
        print(f"   æª”æ¡ˆ: {result['file']}")

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
